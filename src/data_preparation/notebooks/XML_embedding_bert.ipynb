{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaaf0512",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import math\n",
    "import numpy as np\n",
    "import timeit\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "sys.path.append(\"/home/jovyan/20230406_ArticleClassifier/ArticleClassifier\")\n",
    "\n",
    "import src.general.global_variables as gv\n",
    "from src.general.utils import cc_path\n",
    "\n",
    "sys.path.append(gv.PROJECT_PATH)\n",
    "\n",
    "\n",
    "sys.path.append(\n",
    "    os.path.abspath(os.path.join(os.path.dirname('data_loader.py'), os.path.pardir)))\n",
    "from src.data.data_loader import DataLoader as OwnDataLoader\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f36a26b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "from transformers import BertTokenizer, BertModel, AutoTokenizer\n",
    "MAX_LEN = 512\n",
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    #for every sentence...\n",
    "    \n",
    "    for sent in tqdm(data):\n",
    "        # 'encode_plus will':\n",
    "        # (1) Tokenize the sentence\n",
    "        # (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        # (3) Truncate/Pad sentence to max length\n",
    "        # (4) Map tokens to their IDs\n",
    "        # (5) Create attention mask\n",
    "        # (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text = sent,   #preprocess sentence\n",
    "            add_special_tokens = True,         #Add `[CLS]` and `[SEP]`\n",
    "            max_length= MAX_LEN  ,             #Max length to truncate/pad\n",
    "            pad_to_max_length = True,          #pad sentence to max length \n",
    "            return_attention_mask= True,       #Return attention mask \n",
    "            truncation=True\n",
    "        )\n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "        \n",
    "    #convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    \n",
    "    return input_ids,attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d7960d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bert_model(model_path):\n",
    "    do_lower_case = True\n",
    "    if model_path == 'scibert_scivocab_uncased':\n",
    "        model = BertModel.from_pretrained(model_version)\n",
    "    else:\n",
    "        model = torch.load(cc_path(model_path))\n",
    "\n",
    "    return model.base_model\n",
    "\n",
    "# # path options\n",
    "# 'scibert_scivocab_uncased'\n",
    "# f'models/embedders/finetuned_bert_56k_20e_3lay_best_iter.pt'\n",
    "# f'models/embedders/litcovid_finetuned_bert_56k_20e_3lay_best_iter_meta.pt'\n",
    "# f'models/embedders/litcovid_pretrained_best_iter_meta_stopwords.pt'\n",
    "# f'models/baselines/paula_finetuned_bert_56k_10e_tka.pt')\n",
    "\n",
    "model_path = f'models/embedders/litcovid_pretrained_best_iter_meta_stopwords.pt'\n",
    "BERTmodel = load_bert_model(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d5c4865",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1e0fb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "def load_all_canary_data():\n",
    "    # load all the data\n",
    "    loc_dict = {\n",
    "        'processed_csv': cc_path('data/processed/canary/articles_cleaned.csv'),\n",
    "        'abstract_embeddings': cc_path('data/processed/canary/embeddings_fasttext_20230410.csv'),\n",
    "        'keyword_network': cc_path('data/processed/canary/keyword_network_weighted.pickle'),\n",
    "        'author_network': cc_path('data/processed/canary/author_network.pickle')\n",
    "    }\n",
    "    data_loader = OwnDataLoader(loc_dict)\n",
    "    processed_df = data_loader.load_processed_csv()\n",
    "\n",
    "    processed_df['pui'] = processed_df['pui'].astype(str)\n",
    "\n",
    "    label_columns = processed_df.loc[:, ~processed_df.columns.isin(\n",
    "        ['file_name', 'title', 'keywords', 'abstract', 'abstract_2', 'authors', 'organization', 'chemicals',\n",
    "         'num_refs', 'date-delivered', 'labels_m', 'labels_a'])]\n",
    "    label_columns.loc[:, label_columns.columns.difference(['pui'])] = label_columns.loc[\n",
    "                                                                      :, label_columns.columns.difference(['pui'])].astype(str)\n",
    "\n",
    "    with open(cc_path(f'data/train_indices.txt')) as f:\n",
    "        train_puis = f.read().splitlines()\n",
    "    with open(cc_path(f'data/val_indices.txt')) as f:\n",
    "        val_puis = f.read().splitlines()\n",
    "    with open(cc_path(f'data/test_indices.txt')) as f:\n",
    "        test_puis = f.read().splitlines()\n",
    "\n",
    "    return processed_df, train_puis, val_puis, test_puis\n",
    "\n",
    "def generate_canary_embedding_text(df):\n",
    "    df['str_keywords'] = df['keywords'].str.replace('[', ' ').str.replace(']', ' ').str.replace(', ', ' ').str.replace(\"'\", '')\n",
    "    df['embedding_text'] = df['title'] + df['str_keywords'] + df['abstract']\n",
    "\n",
    "    return df\n",
    "\n",
    "processed_df, train_puis, val_puis, test_puis = load_all_canary_data()\n",
    "processed_df = generate_canary_embedding_text(processed_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f113fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading data...\n"
     ]
    }
   ],
   "source": [
    "# Load the custom dataset\n",
    "def load_all_litcovid_data():\n",
    "    loc_dict = {\n",
    "        'processed_csv': cc_path('data/processed/litcovid/litcovid_articles_cleaned.csv'),\n",
    "        'scibert_embeddings': cc_path('data/processed/litcovid/litcovid_embeddings_scibert_finetuned_20230529_meta_stopwords.csv'),\n",
    "        'keyword_network': cc_path('data/processed/litcovid/litcovid_keyword_network_weighted.pickle'),\n",
    "        'xml_embeddings': cc_path('data/processed/litcovid/litcovid_embeddings_xml_20230518_68.ftr'),\n",
    "        'label_network': cc_path('data/processed/litcovid/litcovid_label_network_weighted.pickle')\n",
    "    }\n",
    "    data_loader = OwnDataLoader(loc_dict)\n",
    "    processed_df = data_loader.load_processed_csv()\n",
    "    processed_df.dropna(subset=['abstract'], inplace=True)\n",
    "\n",
    "    label_columns = processed_df.loc[:, ~processed_df.columns.isin(\n",
    "        ['file_name', 'title', 'keywords', 'abstract', 'abstract_2', 'authors', 'organization', 'chemicals',\n",
    "         'num_refs', 'date-delivered', 'labels_m', 'labels_a', 'journal', 'pub_type', 'doi', 'label', 'label_m', 'list_label'])]\n",
    "    label_columns.loc[:, label_columns.columns.difference(['pui'])] = label_columns.loc[:,\n",
    "        label_columns.columns.difference(['pui'])].astype(int)\n",
    "\n",
    "    with open(cc_path(f'data/litcovid_train_indices.txt')) as f:\n",
    "        train_puis = f.read().splitlines()\n",
    "    with open(cc_path(f'data/litcovid_val_indices.txt')) as f:\n",
    "        val_puis = f.read().splitlines()\n",
    "    with open(cc_path(f'data/litcovid_test_indices.txt')) as f:\n",
    "        test_puis = f.read().splitlines()\n",
    "\n",
    "    return processed_df, train_puis, val_puis, test_puis\n",
    "\n",
    "def generate_litcovid_embedding_text(df):\n",
    "\n",
    "    df['str_keywords'] = df['keywords'].str.replace('[', ' ').str.replace(']', ' ').str.replace(', ', ' ').str.replace(\"'\", '')\n",
    "    df['embedding_text'] = df['title'] + \" \" + df['journal'] + \" \" + df['pub_type'].str.replace(';', ' ') + \" \" + df['str_keywords'] + df['abstract']\n",
    "    return df\n",
    "\n",
    "processed_df, train_puis, val_puis, test_puis = load_all_litcovid_data()\n",
    "processed_df = generate_litcovid_embedding_text(processed_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d0f445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24946 [00:00<?, ?it/s]/home/jovyan/.conda/envs/articleclassifier/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 24946/24946 [00:27<00:00, 919.91it/s] \n",
      "100%|██████████| 6236/6236 [00:06<00:00, 969.06it/s] \n",
      "100%|██████████| 2489/2489 [00:02<00:00, 973.47it/s] \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dd7a86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataloader_objects(label_columns, processed_df, train_puis, val_puis, test_puis, batch_size=32):\n",
    "    train_set, train_masks = preprocessing_for_bert(processed_df.loc[processed_df.pui.isin(train_puis), 'embedding_text'])\n",
    "    val_set, val_masks = preprocessing_for_bert(processed_df.loc[processed_df.pui.isin(val_puis), 'embedding_text'])\n",
    "    test_set, test_masks = preprocessing_for_bert(processed_df.loc[processed_df.pui.isin(test_puis), 'embedding_text'])\n",
    "\n",
    "    train_labels = torch.tensor(label_columns.loc[processed_df.pui.isin(train_puis), label_columns.columns.difference(['pui'])].to_numpy(dtype=np.int8))\n",
    "    val_labels = torch.tensor(label_columns.loc[processed_df.pui.isin(val_puis), label_columns.columns.difference(['pui'])].to_numpy(dtype=np.int8))\n",
    "    test_labels = torch.tensor(label_columns.loc[processed_df.pui.isin(test_puis), label_columns.columns.difference(['pui'])].to_numpy(dtype=np.int8))\n",
    "\n",
    "    train_data = TensorDataset(train_set.to(device),train_masks.to(device), train_labels.to(device))\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    val_data = TensorDataset(val_set.to(device), val_masks.to(device), val_labels.to(device))\n",
    "    val_sampler = RandomSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "    test_data = TensorDataset(test_set.to(device), test_masks.to(device), test_labels.to(device))\n",
    "    test_sampler = RandomSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "    return {'train': train_dataloader, 'val': val_dataloader, 'test': test_dataloader}, {'train': train_set, 'val': val_set, 'test': test_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b77358",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataloaders, datasets = generate_dataloader_objects(label_columns, processed_df, train_puis, val_puis, test_puis, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde67e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fcd8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16dad2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [0, 1, 0,  ..., 0, 0, 0],\n",
       "        [0, 1, 0,  ..., 0, 0, 0]], dtype=torch.int8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1a32816",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# create Network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e29aa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BasicModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicModule, self).__init__()\n",
    "        self.model_name = str(type(self))\n",
    "\n",
    "    def load(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "\n",
    "    def save(self, path=None):\n",
    "        if path is None:\n",
    "            raise ValueError('Please specify the saving road!!!')\n",
    "        torch.save(self.state_dict(), path)\n",
    "        return path\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "def get_embedding_layer(embedding_weights):\n",
    "    word_embeddings = nn.Embedding(num_embeddings=embedding_weights.size(0), embedding_dim=embedding_weights.size(1))\n",
    "    word_embeddings.weight.data.copy_(embedding_weights)\n",
    "    word_embeddings.weight.requires_grad = False  # not train\n",
    "    return word_embeddings\n",
    "\n",
    "\n",
    "class Hybrid_XML(BasicModule):\n",
    "    def __init__(self, num_labels=3714, vocab_size=30001, embedding_size=300, embedding_weights=None,\n",
    "                 max_seq=300, hidden_size=256, d_a=256, label_emb=None):\n",
    "        super(Hybrid_XML, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_labels = num_labels\n",
    "        self.max_seq = max_seq\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.bert = BERTmodel\n",
    "        for name, param in self.bert.named_parameters():\n",
    "            param.required_grad = False\n",
    "            if 'layer.11' in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # interaction-attention layer\n",
    "        self.key_layer = torch.nn.Linear(self.embedding_size, self.hidden_size)\n",
    "        self.query_layer = torch.nn.Linear(7, self.hidden_size)\n",
    "\n",
    "        # self-attn layer\n",
    "        self.linear_first = torch.nn.Linear(self.embedding_size, d_a)\n",
    "        self.linear_second = torch.nn.Linear(d_a, self.num_labels)\n",
    "\n",
    "        # weight adaptive layer\n",
    "        self.linear_weight1 = torch.nn.Linear(self.embedding_size, 1)\n",
    "        self.linear_weight2 = torch.nn.Linear(self.embedding_size, 1)\n",
    "\n",
    "        # shared for all attention component\n",
    "        self.linear_final = torch.nn.Linear(768, self.hidden_size)\n",
    "        self.decrease_emb_size = torch.nn.Linear(self.embedding_size, 768)\n",
    "        self.output_layer = torch.nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "        label_embedding = torch.FloatTensor(self.num_labels, 7)\n",
    "        \n",
    "#         label_emb = torch.nn.functional.pad(label_emb, pad=(0, 384-52), mode='constant', value=0)\n",
    "        if label_emb is None:\n",
    "            nn.init.xavier_normal_(label_embedding)\n",
    "        else:\n",
    "            label_embedding.copy_(label_emb)\n",
    "        self.label_embedding = nn.Parameter(label_embedding, requires_grad=False)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        if torch.cuda.is_available():\n",
    "            return (\n",
    "            torch.zeros(2, batch_size, self.hidden_size).cuda(), torch.zeros(2, batch_size, self.hidden_size).cuda())\n",
    "        else:\n",
    "            return (torch.zeros(2, batch_size, self.hidden_size), torch.zeros(2, batch_size, self.hidden_size))\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, embedding_generation=False):\n",
    "\n",
    "#         emb = self.word_embeddings(x)\n",
    "\n",
    "#         hidden_state = self.init_hidden(emb.size(0))\n",
    "#         output, hidden_state = self.lstm(emb, hidden_state)  # [batch,seq,2*hidden]\n",
    "        \n",
    "        output = self.bert(input_ids=input_ids,\n",
    "                            attention_mask = attention_mask)[0]\n",
    "\n",
    "        # get attn_key\n",
    "        attn_key = self.key_layer(output)  # [batch,seq,hidden]\n",
    "        attn_key = attn_key.transpose(1, 2)  # [batch,hidden,seq]\n",
    "        \n",
    "        # get attn_query\n",
    "        label_emb = self.label_embedding.expand(\n",
    "            (attn_key.size(0), self.label_embedding.size(0), self.label_embedding.size(1)))  # [batch,L,label_emb]\n",
    "        label_emb = self.query_layer(label_emb)  # [batch,L,label_emb]\n",
    "        \n",
    "        # attention\n",
    "        similarity = torch.bmm(label_emb, attn_key)  # [batch,L,seq]\n",
    "        similarity = F.softmax(similarity, dim=2)\n",
    "        out1 = torch.bmm(similarity, output)  # [batch,L,label_emb]\n",
    "\n",
    "        # self-attn output\n",
    "        self_attn = torch.tanh(self.linear_first(output))  # [batch,seq,d_a]\n",
    "        self_attn = self.linear_second(self_attn)  # [batch,seq,L]\n",
    "        self_attn = F.softmax(self_attn, dim=1)\n",
    "        self_attn = self_attn.transpose(1, 2)  # [batch,L,seq]\n",
    "        out2 = torch.bmm(self_attn, output)  # [batch,L,hidden]\n",
    "\n",
    "        factor1 = torch.sigmoid(self.linear_weight1(out1))\n",
    "        factor2 = torch.sigmoid(self.linear_weight2(out2))\n",
    "        factor1 = factor1 / (factor1 + factor2)\n",
    "        factor2 = 1 - factor1\n",
    "\n",
    "        out = factor1 * out1 + factor2 * out2\n",
    "        \n",
    "        out = self.decrease_emb_size(out)\n",
    "        \n",
    "        if embedding_generation:\n",
    "            return out\n",
    "        \n",
    "        out = F.relu(self.linear_final(out))\n",
    "        out = torch.sigmoid(self.output_layer(out).squeeze(-1))  # [batch,L]\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4d90929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_embeddings(path, embedding_size):\n",
    "    label_emb = np.zeros(embedding_size)\n",
    "    label_index_mapping = {}\n",
    "    with open(cc_path(path)) as f:\n",
    "        for index, i in enumerate(f.readlines()):\n",
    "            if index == 0:\n",
    "                continue\n",
    "            i = i.rstrip('\\n')\n",
    "            n = i.split(',')[0]\n",
    "            content = i.split(',')[1].split(' ')\n",
    "            label_index_mapping[index-1] = n\n",
    "            label_emb[index-1] = [float(value) for value in content]\n",
    "\n",
    "    # label_emb = (label_emb - label_emb.mean()) / label_emb.std()\n",
    "    label_emb = torch.from_numpy(label_emb).float()\n",
    "    return label_emb\n",
    "\n",
    "label_emb = get_label_embeddings(f'notebooks/litcovid_label_embedding_window3.txt', embedding_size=(7, 7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67c5ed33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0077,  0.0034,  0.0729,  0.1289, -0.1331, -0.1019,  0.0924],\n",
      "        [ 0.1282, -0.0717, -0.0537,  0.1057, -0.0221, -0.0650,  0.0938],\n",
      "        [-0.0694, -0.0260,  0.0412,  0.0144, -0.1186, -0.1352,  0.1047],\n",
      "        [ 0.0725,  0.0966,  0.0110,  0.0909, -0.0488, -0.0137,  0.0826],\n",
      "        [-0.1075, -0.0562, -0.1073, -0.0133,  0.1363, -0.1045, -0.0334],\n",
      "        [-0.0277,  0.1154, -0.0847,  0.0007, -0.0679, -0.1372,  0.0715],\n",
      "        [-0.1251, -0.0627, -0.0005, -0.0042, -0.1094,  0.1374,  0.0712]])\n"
     ]
    }
   ],
   "source": [
    "print(label_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4c372de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # canary\n",
    "# model = Hybrid_XML(num_labels=7, vocab_size=0, embedding_size=768, embedding_weights=0,\n",
    "#                    max_seq=200, hidden_size=16, d_a=52, label_emb=label_emb).to(device)\n",
    "\n",
    "# litcovid \n",
    "model = Hybrid_XML(num_labels=7, vocab_size=0, embedding_size=768, embedding_weights=0,\n",
    "                   max_seq=200, hidden_size=16, d_a=7, label_emb=label_emb).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a52711e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid_XML(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (key_layer): Linear(in_features=768, out_features=16, bias=True)\n",
      "  (query_layer): Linear(in_features=7, out_features=16, bias=True)\n",
      "  (linear_first): Linear(in_features=768, out_features=7, bias=True)\n",
      "  (linear_second): Linear(in_features=7, out_features=7, bias=True)\n",
      "  (linear_weight1): Linear(in_features=768, out_features=1, bias=True)\n",
      "  (linear_weight2): Linear(in_features=768, out_features=1, bias=True)\n",
      "  (linear_final): Linear(in_features=768, out_features=16, bias=True)\n",
      "  (decrease_emb_size): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (output_layer): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cce04c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "import copy\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=0.00001, weight_decay=1e-4)\n",
    "criterion = torch.nn.BCELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b192440f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1, train_loss = 0.0000, test_loss = 0.0000, train_f1 = 0.0000, test_f1 = 0.0000, train_loss: 0.004096423275768757: : 780it [05:47,  2.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current test score:  0.8868498086016662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2, train_loss = 0.0059, test_loss = 0.0043, train_f1 = 0.8059, test_f1 = 0.8868, train_loss: 0.0040702312253415585: : 780it [05:48,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current test score:  0.8829308909242298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3, train_loss = 0.0027, test_loss = 0.0043, train_f1 = 0.9209, test_f1 = 0.8829, train_loss: 0.004857152700424194: : 780it [05:48,  2.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current test score:  0.8861720538156644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4, train_loss = 0.0027, test_loss = 0.0041, train_f1 = 0.9180, test_f1 = 0.8862, train_loss: 0.0035712234675884247: : 780it [05:48,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current test score:  0.8792535675082327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5, train_loss = 0.0029, test_loss = 0.0044, train_f1 = 0.9126, test_f1 = 0.8793, train_loss: 0.0022972405422478914: : 780it [05:48,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current test score:  0.880641592920354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6, train_loss = 0.0029, test_loss = 0.0043, train_f1 = 0.9098, test_f1 = 0.8806, train_loss: 0.0076494961977005005: : 780it [05:48,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current test score:  0.8902195608782435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 7, train_loss = 0.0030, test_loss = 0.0037, train_f1 = 0.9082, test_f1 = 0.8902, train_loss: 0.0035466747358441353: : 780it [05:48,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current test score:  0.885441660621617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 8, train_loss = 0.0031, test_loss = 0.0041, train_f1 = 0.9048, test_f1 = 0.8854, train_loss: 0.00452516321092844: : 780it [05:48,  2.24it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current test score:  0.8921067575241342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 9, train_loss = 0.0032, test_loss = 0.0038, train_f1 = 0.9019, test_f1 = 0.8921, train_loss: 0.00777384964749217: : 780it [05:48,  2.24it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current test score:  0.8889526542324246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 10, train_loss = 0.0032, test_loss = 0.0037, train_f1 = 0.9018, test_f1 = 0.8890, train_loss: 0.002711153356358409: : 780it [05:48,  2.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current test score:  0.8774193548387097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 11, train_loss = 0.0032, test_loss = 0.0043, train_f1 = 0.9011, test_f1 = 0.8774, train_loss: 0.004980372730642557: : 780it [05:48,  2.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current test score:  0.8878552385274263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 12, train_loss = 0.0033, test_loss = 0.0039, train_f1 = 0.8987, test_f1 = 0.8879, train_loss: 0.006095762364566326: : 780it [05:48,  2.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current test score:  0.8863328822733424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 13, train_loss = 0.0033, test_loss = 0.0039, train_f1 = 0.8987, test_f1 = 0.8863, train_loss: 0.005344081670045853: : 780it [05:48,  2.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current test score:  0.8897507557177894\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epoch = 60\n",
    "best_acc = 0.0\n",
    "pre_acc = 0.0\n",
    "\n",
    "# if not os.path.isdir('./rcv_log'):\n",
    "#     os.makedirs('./rcv_log')\n",
    "# trace_file='./rcv_log/trace_rcv.txt'\n",
    "\n",
    "\n",
    "\n",
    "num_labels = 7\n",
    "\n",
    "def train_epoch(model, dataloader, dataset, pbar_description, optimizer, num_labels, batch_size):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_score = 0\n",
    "    predictions = np.zeros((len(dataset), num_labels))\n",
    "    real_labels = np.zeros((len(dataset), num_labels))\n",
    "    for i, (data, atts, labels) in (pbar := tqdm(enumerate(dataloader), position=0)):\n",
    "        # print('new batch: ', i)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # data = data.cuda()\n",
    "        # labels = labels.cuda()\n",
    "\n",
    "        pred = model(data, atts)\n",
    "        loss = criterion(pred, labels.float()) / pred.size(0)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += float(loss)\n",
    "#         train_score += f1_score(labels.detach().cpu().numpy(), np.round(pred.detach().cpu().numpy()), average='macro', zero_division=0)\n",
    "        predictions[i*batch_size: (i+1)*batch_size, :] = np.round(pred.detach().cpu().numpy())\n",
    "        real_labels[i*batch_size: (i+1)*batch_size, :] = labels.detach().cpu().numpy()\n",
    "        pbar.set_description(pbar_description + f', train_loss: {loss}')\n",
    "\n",
    "    train_score = f1_score(real_labels, predictions, average='micro', zero_division=0)\n",
    "    train_loss /= i + 1\n",
    "\n",
    "    return train_loss, train_score\n",
    "\n",
    "def evaluation(model, dataloader, dataset, pbar_description, optimizer, num_labels, batch_size):\n",
    "        test_loss = 0\n",
    "        test_predictions = np.zeros((len(dataset), num_labels))\n",
    "        test_real_labels = np.zeros((len(dataset), num_labels))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (data, atts, labels) in enumerate(dataloader):\n",
    "                # data = data.cuda()\n",
    "                # labels = labels.cuda()\n",
    "                pred = model(data, atts)\n",
    "                loss = criterion(pred, labels.float()) / pred.size(0)\n",
    "\n",
    "                # metric\n",
    "                labels_cpu = labels.data.cpu().numpy()\n",
    "                pred_cpu = np.round(pred.data.cpu().numpy())\n",
    "\n",
    "                test_loss += float(loss)\n",
    "                test_predictions[i*batch_size: (i+1)*batch_size, :] = pred_cpu\n",
    "                test_real_labels[i*batch_size: (i+1)*batch_size, :] = labels_cpu\n",
    "\n",
    "        batch_num = i + 1\n",
    "        test_loss /= batch_num\n",
    "    #     test_score /= batch_num\n",
    "        test_score = f1_score(test_real_labels, test_predictions, average='micro', zero_division=0)\n",
    "\n",
    "        return test_score, test_loss, test_predictions, test_real_labels\n",
    "\n",
    "\n",
    "def train_model(model, dataloaders, datasets, optimizer, num_labels, batch_size):\n",
    "    best_val_score = 0\n",
    "    not_improved = 0\n",
    "\n",
    "    val_loss = 0\n",
    "    val_score  = 0\n",
    "\n",
    "    for ep in range(1, epoch + 1):\n",
    "\n",
    "        pbar_description = f\"epoch {ep}, train_loss = {train_loss:.4f}, test_loss = {val_loss:.4f}, train_f1 = {train_score:.4f}, val_f1 = {val_score:.4f}\"\n",
    "\n",
    "        train_loss, train_score, batch_num = train_epoch(model, dataloaders['train'], datasets['train'], pbar_description, optimizer, num_labels, batch_size)\n",
    "\n",
    "        val_score, val_loss, _, _ = evaluation(model, dataloaders['val'], datasets['val'], pbar_description, optimizer, num_labels, batch_size)\n",
    "\n",
    "        print('The current test score: ', val_score)\n",
    "        if val_score > best_val_score:\n",
    "            best_val_score = val_score\n",
    "            best_model = copy.deepcopy(model)\n",
    "            not_improved = 0\n",
    "        else:\n",
    "            not_improved += 1\n",
    "\n",
    "        if not_improved == 5:\n",
    "            break\n",
    "\n",
    "        return best_model\n",
    "\n",
    "best_model = train_model(model, dataloaders, datasets, optimizer, num_labels, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa3c97bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "78it [00:22,  3.48it/s]\n"
     ]
    }
   ],
   "source": [
    "test_score, test_loss, test_predictions, test_real_labels = evaluation(model, dataloaders['test'], datasets['test'], pbar_description, optimizer, num_labels, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcd1180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     test_score /= batch_num\n",
    "macro_f1_test_score = f1_score(test_real_labels, test_predictions, average='macro', zero_division=0)\n",
    "micro_f1_test_score = f1_score(test_real_labels, test_predictions, average='micro', zero_division=0)\n",
    "macro_recall_test_score = recall_score(test_real_labels, test_predictions, average='macro', zero_division=0)\n",
    "micro_recall_test_score = recall_score(test_real_labels, test_predictions, average='micro', zero_division=0)\n",
    "macro_precision_test_score = precision_score(test_real_labels, test_predictions, average='macro', zero_division=0)\n",
    "micro_precision_test_score = precision_score(test_real_labels, test_predictions, average='micro', zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "037348be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8408344272973511 0.8876959043060806 0.8134234067703165 0.8970251716247141 0.9255555555555556 0.870201096892139\n"
     ]
    }
   ],
   "source": [
    "print(macro_f1_test_score, macro_recall_test_score, macro_precision_test_score, micro_f1_test_score, micro_recall_test_score, micro_precision_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1f200b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34de6ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model, cc_path(f'models/xml_embedding/litcovid_xlm_embedder_20230529_stopwords.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fabdeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load(cc_path(f'models/xml_embedding/litcovid_xlm_embedder_20230518_all_data.pt'), map_location=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a92f71fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_data, train_sampler, train_dataloader, val_data, val_sampler, val_dataloader, test_data, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "515bebfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/33671 [00:00<?, ?it/s]/home/jovyan/.conda/envs/articleclassifier/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 33671/33671 [00:37<00:00, 905.97it/s] \n"
     ]
    }
   ],
   "source": [
    "emb_batch_size = 256\n",
    "# abstracts_to_embed = np.array(processed_df['embedding_text'], dtype=int)\n",
    "\n",
    "# embedding_data = data_utils.TensorDataset(torch.from_numpy(abstracts_to_embed).type(torch.LongTensor), \n",
    "#                                           torch.from_numpy(puis_to_embed).type(torch.LongTensor))\n",
    "# final_data = data_utils.DataLoader(embedding_data, emb_batch_size, drop_last=False)\n",
    "\n",
    "\n",
    "full_set = processed_df.dropna(subset=['embedding_text'])\n",
    "\n",
    "puis_to_embed = np.array(full_set.loc[:, 'pui'].to_list(), dtype=int)\n",
    "\n",
    "final_set, final_masks = preprocessing_for_bert(full_set.loc[:, 'embedding_text'])\n",
    "final_data = TensorDataset(final_set.to(device), final_masks.to(device),  torch.from_numpy(puis_to_embed).type(torch.LongTensor).to(device))\n",
    "final_dataloader = DataLoader(final_data, batch_size=emb_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e7f52f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "088be4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_columns =  [f'd_{i}' for i in range(7*768)]\n",
    "xml_embedding_df = pd.DataFrame(columns=embedding_columns, index=full_set['pui'].to_numpy(dtype=int)).astype(np.float16)\n",
    "# xml_embedding_df['embedding'] = xml_embedding_df['embedding'].astype(object)\n",
    "np.set_printoptions(threshold = 100000000000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3a06c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [06:19<00:00,  2.88s/it]\n"
     ]
    }
   ],
   "source": [
    "num_of_embedding_dim = 104\n",
    "best_model.eval()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (data, att_masks, pui) in enumerate(tqdm(final_dataloader)):\n",
    "        pred = best_model(data, att_masks, embedding_generation=True)\n",
    "        right_puis =  list(pui.detach().cpu().numpy())\n",
    "        numpy_preds = pred.detach().cpu().numpy()\n",
    "        # print(numpy_preds.reshape(numpy_preds.shape[0], numpy_preds.shape[1] * numpy_preds.shape[2]).shape)\n",
    "        # for idx_batch in range(numpy_preds.shape[0]):\n",
    "        xml_embedding_df.loc[right_puis, :] = numpy_preds.reshape(numpy_preds.shape[0], numpy_preds.shape[1] * numpy_preds.shape[2])\n",
    "                \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d24c2465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1450/2776240131.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  xml_embedding_df.reset_index(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "xml_embedding_df.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58ff65bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index     6.245314e+08\n",
       "d_0       1.072229e+00\n",
       "d_1       1.634674e-01\n",
       "d_2      -1.355221e+00\n",
       "d_3       1.438000e+00\n",
       "              ...     \n",
       "d_3531    1.978726e+00\n",
       "d_3532   -3.448576e-01\n",
       "d_3533   -1.053946e+00\n",
       "d_3534    7.694158e+00\n",
       "d_3535    9.000473e+00\n",
       "Name: 0, Length: 3537, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_embedding_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73b425ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xml_embedding_df.to_feather(cc_path('data/processed/litcovid/litcovid_embeddings_xml_20230529_768.ftr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "87c6a3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.width = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8d6d18c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d_0</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>d_5</th>\n",
       "      <th>d_6</th>\n",
       "      <th>d_7</th>\n",
       "      <th>d_8</th>\n",
       "      <th>d_9</th>\n",
       "      <th>...</th>\n",
       "      <th>d_42</th>\n",
       "      <th>d_43</th>\n",
       "      <th>d_44</th>\n",
       "      <th>d_45</th>\n",
       "      <th>d_46</th>\n",
       "      <th>d_47</th>\n",
       "      <th>d_48</th>\n",
       "      <th>d_49</th>\n",
       "      <th>d_50</th>\n",
       "      <th>d_51</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>624531411</th>\n",
       "      <td>[-0.016383378, -0.11197758, -0.037253555, -0.2...</td>\n",
       "      <td>[0.010617012, -0.09705223, -0.01200425, -0.120...</td>\n",
       "      <td>[0.028110279, -0.08243386, -0.016131265, -0.18...</td>\n",
       "      <td>[0.0022158436, -0.11320748, -0.010183357, -0.1...</td>\n",
       "      <td>[0.005926796, -0.10500478, -0.010475147, -0.12...</td>\n",
       "      <td>[0.0113420375, -0.09601049, -0.014044623, -0.1...</td>\n",
       "      <td>[0.016601732, -0.09619771, -0.009100688, -0.14...</td>\n",
       "      <td>[0.11544543, -0.005443737, -0.005527966, -0.18...</td>\n",
       "      <td>[0.02586015, -0.086351044, -0.010884014, -0.16...</td>\n",
       "      <td>[0.009882777, -0.09707534, -0.021866538, -0.18...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.18155344, 0.026362767, -0.01475883, -0.2147...</td>\n",
       "      <td>[0.0069437977, -0.09799223, -0.02816795, -0.23...</td>\n",
       "      <td>[0.18892401, 0.041114308, -0.00097465096, -0.1...</td>\n",
       "      <td>[0.11469094, -0.015334345, 0.006475803, -0.087...</td>\n",
       "      <td>[0.027374564, -0.08158906, -0.01762917, -0.206...</td>\n",
       "      <td>[0.04705087, -0.065758094, -0.018019354, -0.21...</td>\n",
       "      <td>[0.3474198, 0.15047382, 0.0039090053, -0.13961...</td>\n",
       "      <td>[0.11787519, -0.0064049475, -0.0025091413, -0....</td>\n",
       "      <td>[0.069682136, -0.054890573, -0.011223326, -0.1...</td>\n",
       "      <td>[-0.011351064, -0.100169934, -0.034848228, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625340088</th>\n",
       "      <td>[0.06821822, -0.05620522, -0.065800205, -0.222...</td>\n",
       "      <td>[0.10693053, -0.017095909, -0.044121563, -0.09...</td>\n",
       "      <td>[0.10340122, -0.019759048, -0.055330824, -0.16...</td>\n",
       "      <td>[0.044841465, -0.09599258, -0.047074445, -0.14...</td>\n",
       "      <td>[0.042860292, -0.08393292, -0.043275695, -0.10...</td>\n",
       "      <td>[0.038487554, -0.07838504, -0.053617507, -0.16...</td>\n",
       "      <td>[0.05473292, -0.075368986, -0.042547263, -0.12...</td>\n",
       "      <td>[0.18368945, 0.09457679, -0.079515636, -0.1797...</td>\n",
       "      <td>[0.1327912, 0.019922812, -0.06676929, -0.15574...</td>\n",
       "      <td>[0.1424106, 0.018941574, -0.052223623, -0.1458...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.24319527, 0.12634419, -0.05570019, -0.18991...</td>\n",
       "      <td>[0.12861286, 0.009207115, -0.063367575, -0.198...</td>\n",
       "      <td>[0.24438602, 0.14896263, -0.07208059, -0.14055...</td>\n",
       "      <td>[0.1749304, 0.08700106, -0.05070314, -0.066946...</td>\n",
       "      <td>[0.100104034, -0.02964319, -0.0711903, -0.2004...</td>\n",
       "      <td>[0.13491955, 0.01794957, -0.067108504, -0.1916...</td>\n",
       "      <td>[0.41909015, 0.33678198, -0.07781066, -0.12629...</td>\n",
       "      <td>[0.17230125, 0.06469397, -0.08423577, -0.16590...</td>\n",
       "      <td>[0.15537813, 0.036530755, -0.06381854, -0.1704...</td>\n",
       "      <td>[0.02809963, -0.08056283, -0.07836892, -0.2682...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625805682</th>\n",
       "      <td>[0.028528668, -0.060206894, 0.07548873, -0.348...</td>\n",
       "      <td>[0.042353593, -0.07522996, 0.045501076, -0.324...</td>\n",
       "      <td>[0.043712128, -0.07069669, 0.053283475, -0.341...</td>\n",
       "      <td>[0.0912251, -0.05125811, 0.106232695, -0.31711...</td>\n",
       "      <td>[0.071796775, -0.050604787, 0.09280021, -0.337...</td>\n",
       "      <td>[0.076959535, -0.04865962, 0.090105645, -0.341...</td>\n",
       "      <td>[0.077228464, -0.051366046, 0.099378556, -0.32...</td>\n",
       "      <td>[0.051017053, -0.06567388, 0.10099919, -0.2431...</td>\n",
       "      <td>[0.04003511, -0.047263972, 0.09858238, -0.3281...</td>\n",
       "      <td>[0.054769102, -0.05078943, 0.08115368, -0.2868...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.18210159, 0.063631855, 0.05955499, -0.32273...</td>\n",
       "      <td>[0.057935737, -0.034007467, 0.08288933, -0.324...</td>\n",
       "      <td>[0.21283509, 0.080757335, 0.084314525, -0.2392...</td>\n",
       "      <td>[0.099942975, -0.049863294, 0.13453224, -0.256...</td>\n",
       "      <td>[0.046176944, -0.0769093, 0.06805101, -0.29428...</td>\n",
       "      <td>[0.037154716, -0.05500753, 0.088356555, -0.317...</td>\n",
       "      <td>[0.33445185, 0.20716941, 0.06621084, -0.234082...</td>\n",
       "      <td>[0.07703457, -0.032216277, 0.1353079, -0.22812...</td>\n",
       "      <td>[0.06902366, -0.051700268, 0.06834428, -0.3177...</td>\n",
       "      <td>[0.03009884, -0.06154553, 0.08346843, -0.34348...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626662493</th>\n",
       "      <td>[0.059955165, -0.02994335, -0.03311316, -0.143...</td>\n",
       "      <td>[0.10139962, 0.009669423, -0.022478202, -0.038...</td>\n",
       "      <td>[0.0790334, -0.01266022, -0.02287779, -0.10124...</td>\n",
       "      <td>[0.030228794, -0.07793985, -0.028091416, -0.13...</td>\n",
       "      <td>[0.036995795, -0.06270175, -0.0315825, -0.1039...</td>\n",
       "      <td>[0.027064249, -0.0709749, -0.031441428, -0.146...</td>\n",
       "      <td>[0.043158136, -0.059222713, -0.027855176, -0.1...</td>\n",
       "      <td>[0.13726412, 0.05951462, 0.010710327, -0.14864...</td>\n",
       "      <td>[0.083741456, 0.0010779575, 0.00037083589, -0....</td>\n",
       "      <td>[0.12572579, 0.037525304, -0.019792218, -0.073...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.15025625, 0.0521947, -0.026349314, -0.17327...</td>\n",
       "      <td>[0.11670811, 0.030976295, -0.02641234, -0.1190...</td>\n",
       "      <td>[0.19017154, 0.094812796, -0.003392328, -0.117...</td>\n",
       "      <td>[0.13538006, 0.04384046, 0.006757291, -0.05630...</td>\n",
       "      <td>[0.07334437, -0.024985045, -0.021543493, -0.13...</td>\n",
       "      <td>[0.097782776, 0.006623201, -0.026629627, -0.12...</td>\n",
       "      <td>[0.3373683, 0.24063022, -0.0021311226, -0.1236...</td>\n",
       "      <td>[0.15725388, 0.042757872, -0.008270113, -0.101...</td>\n",
       "      <td>[0.12123768, 0.029323883, -0.025152106, -0.115...</td>\n",
       "      <td>[0.022587577, -0.067165665, -0.03826805, -0.19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626822402</th>\n",
       "      <td>[0.27494726, -0.061647326, 0.18791562, -0.2993...</td>\n",
       "      <td>[0.3305084, -0.046140015, 0.18127444, -0.38189...</td>\n",
       "      <td>[0.30549046, -0.052407146, 0.18561853, -0.3563...</td>\n",
       "      <td>[0.33089703, -0.05900829, 0.19056454, -0.28315...</td>\n",
       "      <td>[0.3212989, -0.053632278, 0.18682936, -0.28965...</td>\n",
       "      <td>[0.33208808, -0.05664587, 0.18075578, -0.28696...</td>\n",
       "      <td>[0.32238388, -0.05355747, 0.18911934, -0.28512...</td>\n",
       "      <td>[0.2843451, -0.075362384, 0.17306942, -0.26655...</td>\n",
       "      <td>[0.26771063, -0.066218354, 0.17441167, -0.2618...</td>\n",
       "      <td>[0.28459507, -0.07167025, 0.17930306, -0.29999...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.34554872, -0.114436746, 0.16829908, -0.2782...</td>\n",
       "      <td>[0.25267428, -0.080995634, 0.17711401, -0.2627...</td>\n",
       "      <td>[0.3340537, -0.093892336, 0.1561747, -0.314642...</td>\n",
       "      <td>[0.3147732, -0.065012425, 0.17388539, -0.25686...</td>\n",
       "      <td>[0.29047766, -0.05335179, 0.18333776, -0.34369...</td>\n",
       "      <td>[0.2530854, -0.07009541, 0.18177888, -0.267674...</td>\n",
       "      <td>[0.3673926, -0.15189306, 0.14653346, -0.250014...</td>\n",
       "      <td>[0.27901053, -0.060020022, 0.20105505, -0.2875...</td>\n",
       "      <td>[0.2838174, -0.074696735, 0.17902729, -0.29193...</td>\n",
       "      <td>[0.25677127, -0.058577746, 0.19382006, -0.2792...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011621972</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011622024</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011622065</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011626864</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011632199</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117310 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          d_0   \n",
       "624531411   [-0.016383378, -0.11197758, -0.037253555, -0.2...  \\\n",
       "625340088   [0.06821822, -0.05620522, -0.065800205, -0.222...   \n",
       "625805682   [0.028528668, -0.060206894, 0.07548873, -0.348...   \n",
       "626662493   [0.059955165, -0.02994335, -0.03311316, -0.143...   \n",
       "626822402   [0.27494726, -0.061647326, 0.18791562, -0.2993...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                          d_1   \n",
       "624531411   [0.010617012, -0.09705223, -0.01200425, -0.120...  \\\n",
       "625340088   [0.10693053, -0.017095909, -0.044121563, -0.09...   \n",
       "625805682   [0.042353593, -0.07522996, 0.045501076, -0.324...   \n",
       "626662493   [0.10139962, 0.009669423, -0.022478202, -0.038...   \n",
       "626822402   [0.3305084, -0.046140015, 0.18127444, -0.38189...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                          d_2   \n",
       "624531411   [0.028110279, -0.08243386, -0.016131265, -0.18...  \\\n",
       "625340088   [0.10340122, -0.019759048, -0.055330824, -0.16...   \n",
       "625805682   [0.043712128, -0.07069669, 0.053283475, -0.341...   \n",
       "626662493   [0.0790334, -0.01266022, -0.02287779, -0.10124...   \n",
       "626822402   [0.30549046, -0.052407146, 0.18561853, -0.3563...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                          d_3   \n",
       "624531411   [0.0022158436, -0.11320748, -0.010183357, -0.1...  \\\n",
       "625340088   [0.044841465, -0.09599258, -0.047074445, -0.14...   \n",
       "625805682   [0.0912251, -0.05125811, 0.106232695, -0.31711...   \n",
       "626662493   [0.030228794, -0.07793985, -0.028091416, -0.13...   \n",
       "626822402   [0.33089703, -0.05900829, 0.19056454, -0.28315...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                          d_4   \n",
       "624531411   [0.005926796, -0.10500478, -0.010475147, -0.12...  \\\n",
       "625340088   [0.042860292, -0.08393292, -0.043275695, -0.10...   \n",
       "625805682   [0.071796775, -0.050604787, 0.09280021, -0.337...   \n",
       "626662493   [0.036995795, -0.06270175, -0.0315825, -0.1039...   \n",
       "626822402   [0.3212989, -0.053632278, 0.18682936, -0.28965...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                          d_5   \n",
       "624531411   [0.0113420375, -0.09601049, -0.014044623, -0.1...  \\\n",
       "625340088   [0.038487554, -0.07838504, -0.053617507, -0.16...   \n",
       "625805682   [0.076959535, -0.04865962, 0.090105645, -0.341...   \n",
       "626662493   [0.027064249, -0.0709749, -0.031441428, -0.146...   \n",
       "626822402   [0.33208808, -0.05664587, 0.18075578, -0.28696...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                          d_6   \n",
       "624531411   [0.016601732, -0.09619771, -0.009100688, -0.14...  \\\n",
       "625340088   [0.05473292, -0.075368986, -0.042547263, -0.12...   \n",
       "625805682   [0.077228464, -0.051366046, 0.099378556, -0.32...   \n",
       "626662493   [0.043158136, -0.059222713, -0.027855176, -0.1...   \n",
       "626822402   [0.32238388, -0.05355747, 0.18911934, -0.28512...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                          d_7   \n",
       "624531411   [0.11544543, -0.005443737, -0.005527966, -0.18...  \\\n",
       "625340088   [0.18368945, 0.09457679, -0.079515636, -0.1797...   \n",
       "625805682   [0.051017053, -0.06567388, 0.10099919, -0.2431...   \n",
       "626662493   [0.13726412, 0.05951462, 0.010710327, -0.14864...   \n",
       "626822402   [0.2843451, -0.075362384, 0.17306942, -0.26655...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                          d_8   \n",
       "624531411   [0.02586015, -0.086351044, -0.010884014, -0.16...  \\\n",
       "625340088   [0.1327912, 0.019922812, -0.06676929, -0.15574...   \n",
       "625805682   [0.04003511, -0.047263972, 0.09858238, -0.3281...   \n",
       "626662493   [0.083741456, 0.0010779575, 0.00037083589, -0....   \n",
       "626822402   [0.26771063, -0.066218354, 0.17441167, -0.2618...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                          d_9  ...   \n",
       "624531411   [0.009882777, -0.09707534, -0.021866538, -0.18...  ...  \\\n",
       "625340088   [0.1424106, 0.018941574, -0.052223623, -0.1458...  ...   \n",
       "625805682   [0.054769102, -0.05078943, 0.08115368, -0.2868...  ...   \n",
       "626662493   [0.12572579, 0.037525304, -0.019792218, -0.073...  ...   \n",
       "626822402   [0.28459507, -0.07167025, 0.17930306, -0.29999...  ...   \n",
       "...                                                       ...  ...   \n",
       "2011621972                                                NaN  ...   \n",
       "2011622024                                                NaN  ...   \n",
       "2011622065                                                NaN  ...   \n",
       "2011626864                                                NaN  ...   \n",
       "2011632199                                                NaN  ...   \n",
       "\n",
       "                                                         d_42   \n",
       "624531411   [0.18155344, 0.026362767, -0.01475883, -0.2147...  \\\n",
       "625340088   [0.24319527, 0.12634419, -0.05570019, -0.18991...   \n",
       "625805682   [0.18210159, 0.063631855, 0.05955499, -0.32273...   \n",
       "626662493   [0.15025625, 0.0521947, -0.026349314, -0.17327...   \n",
       "626822402   [0.34554872, -0.114436746, 0.16829908, -0.2782...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                         d_43   \n",
       "624531411   [0.0069437977, -0.09799223, -0.02816795, -0.23...  \\\n",
       "625340088   [0.12861286, 0.009207115, -0.063367575, -0.198...   \n",
       "625805682   [0.057935737, -0.034007467, 0.08288933, -0.324...   \n",
       "626662493   [0.11670811, 0.030976295, -0.02641234, -0.1190...   \n",
       "626822402   [0.25267428, -0.080995634, 0.17711401, -0.2627...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                         d_44   \n",
       "624531411   [0.18892401, 0.041114308, -0.00097465096, -0.1...  \\\n",
       "625340088   [0.24438602, 0.14896263, -0.07208059, -0.14055...   \n",
       "625805682   [0.21283509, 0.080757335, 0.084314525, -0.2392...   \n",
       "626662493   [0.19017154, 0.094812796, -0.003392328, -0.117...   \n",
       "626822402   [0.3340537, -0.093892336, 0.1561747, -0.314642...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                         d_45   \n",
       "624531411   [0.11469094, -0.015334345, 0.006475803, -0.087...  \\\n",
       "625340088   [0.1749304, 0.08700106, -0.05070314, -0.066946...   \n",
       "625805682   [0.099942975, -0.049863294, 0.13453224, -0.256...   \n",
       "626662493   [0.13538006, 0.04384046, 0.006757291, -0.05630...   \n",
       "626822402   [0.3147732, -0.065012425, 0.17388539, -0.25686...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                         d_46   \n",
       "624531411   [0.027374564, -0.08158906, -0.01762917, -0.206...  \\\n",
       "625340088   [0.100104034, -0.02964319, -0.0711903, -0.2004...   \n",
       "625805682   [0.046176944, -0.0769093, 0.06805101, -0.29428...   \n",
       "626662493   [0.07334437, -0.024985045, -0.021543493, -0.13...   \n",
       "626822402   [0.29047766, -0.05335179, 0.18333776, -0.34369...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                         d_47   \n",
       "624531411   [0.04705087, -0.065758094, -0.018019354, -0.21...  \\\n",
       "625340088   [0.13491955, 0.01794957, -0.067108504, -0.1916...   \n",
       "625805682   [0.037154716, -0.05500753, 0.088356555, -0.317...   \n",
       "626662493   [0.097782776, 0.006623201, -0.026629627, -0.12...   \n",
       "626822402   [0.2530854, -0.07009541, 0.18177888, -0.267674...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                         d_48   \n",
       "624531411   [0.3474198, 0.15047382, 0.0039090053, -0.13961...  \\\n",
       "625340088   [0.41909015, 0.33678198, -0.07781066, -0.12629...   \n",
       "625805682   [0.33445185, 0.20716941, 0.06621084, -0.234082...   \n",
       "626662493   [0.3373683, 0.24063022, -0.0021311226, -0.1236...   \n",
       "626822402   [0.3673926, -0.15189306, 0.14653346, -0.250014...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                         d_49   \n",
       "624531411   [0.11787519, -0.0064049475, -0.0025091413, -0....  \\\n",
       "625340088   [0.17230125, 0.06469397, -0.08423577, -0.16590...   \n",
       "625805682   [0.07703457, -0.032216277, 0.1353079, -0.22812...   \n",
       "626662493   [0.15725388, 0.042757872, -0.008270113, -0.101...   \n",
       "626822402   [0.27901053, -0.060020022, 0.20105505, -0.2875...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                         d_50   \n",
       "624531411   [0.069682136, -0.054890573, -0.011223326, -0.1...  \\\n",
       "625340088   [0.15537813, 0.036530755, -0.06381854, -0.1704...   \n",
       "625805682   [0.06902366, -0.051700268, 0.06834428, -0.3177...   \n",
       "626662493   [0.12123768, 0.029323883, -0.025152106, -0.115...   \n",
       "626822402   [0.2838174, -0.074696735, 0.17902729, -0.29193...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                         d_51  \n",
       "624531411   [-0.011351064, -0.100169934, -0.034848228, -0....  \n",
       "625340088   [0.02809963, -0.08056283, -0.07836892, -0.2682...  \n",
       "625805682   [0.03009884, -0.06154553, 0.08346843, -0.34348...  \n",
       "626662493   [0.022587577, -0.067165665, -0.03826805, -0.19...  \n",
       "626822402   [0.25677127, -0.058577746, 0.19382006, -0.2792...  \n",
       "...                                                       ...  \n",
       "2011621972                                                NaN  \n",
       "2011622024                                                NaN  \n",
       "2011622065                                                NaN  \n",
       "2011626864                                                NaN  \n",
       "2011632199                                                NaN  \n",
       "\n",
       "[117310 rows x 52 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_embedding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7eee1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-articleclassifier [Python]",
   "language": "python",
   "name": "conda-env-.conda-articleclassifier-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
