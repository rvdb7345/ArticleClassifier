{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21ddb39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "sys.path.append(\"/home/jovyan/20230406_ArticleClassifier/ArticleClassifier\")\n",
    "\n",
    "import src.general.global_variables as gv\n",
    "sys.path.append(\n",
    "    os.path.abspath(os.path.join(os.path.dirname('data_loader.py'), os.path.pardir)))\n",
    "from src.data.data_loader import DataLoader\n",
    "\n",
    "from src.general.utils import cc_path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "652ecf25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "model_version = 'scibert_scivocab_uncased'\n",
    "do_lower_case = True\n",
    "# model = BertModel.from_pretrained(model_version)\n",
    "# model = torch.load(cc_path(f'models/embedders/finetuned_bert_56k_20e_3lay_best_iter.pt'))\n",
    "model = torch.load(cc_path(f'models/baselines/paula_finetuned_bert_56k_10e_tka.pt'))\n",
    "model = model.base_model\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "486a3932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def embed_text(text, model):\n",
    "    # print(text)\n",
    "    encoded_text = tokenizer.encode(text, max_length=512, truncation=True)\n",
    "    input_ids = torch.tensor(encoded_text).unsqueeze(0).to(device)  # Batch size 1\n",
    "    outputs = model(input_ids)\n",
    "    last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "    return last_hidden_states \n",
    "\n",
    "def get_similarity(em, em2):\n",
    "    return cosine_similarity(em.detach().cpu().numpy(), em2.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce93cc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity for Coronavirus and Flu:[[0.13712637]]\n",
      "Similarity for Coronavirus and MERs:[[0.13969503]]\n",
      "Similarity for Coronavirus and COVID-2019:[[0.03997703]]\n",
      "Similarity for Coronavirus and Bog:[[0.14438884]]\n"
     ]
    }
   ],
   "source": [
    "# We will use a mean of all word embeddings. To do that we will take mean over dimension 1 which is the sequence length.\n",
    "coronavirus_em = embed_text([\"Coronavirus\"]*50000, model).mean(1)\n",
    "mers_em = embed_text(\"Middle East Respiratory Virus\", model).mean(1)\n",
    "flu_em = embed_text(\"Flu\", model).mean(1)\n",
    "bog_em = embed_text(\"Bog\", model).mean(1)\n",
    "covid_2019 = embed_text(\"COVID-2019\", model).mean(1)\n",
    "print(\"Similarity for Coronavirus and Flu:\" + str(get_similarity(coronavirus_em, flu_em)))\n",
    "print(\"Similarity for Coronavirus and MERs:\" + str(get_similarity(coronavirus_em, mers_em)))\n",
    "print(\"Similarity for Coronavirus and COVID-2019:\" + str(get_similarity(coronavirus_em, covid_2019)))\n",
    "print(\"Similarity for Coronavirus and Bog:\" + str(get_similarity(coronavirus_em, bog_em)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb936e71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6148b1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scibert_embedding(emb_dat, embedding_dim):\n",
    "    \"\"\"Create the SciBERT embedding\"\"\"\n",
    "\n",
    "    print('Initiating DataFrame for saving embedding...')\n",
    "    embedding_cols = [f'd{i}' for i in range(embedding_dim)]\n",
    "    embedded_df = pd.DataFrame(columns=['pui'] + embedding_cols)\n",
    "    embedded_df['pui'] = emb_dat.loc[:, 'pui']\n",
    "    embedded_df.set_index('pui', inplace=True)\n",
    "\n",
    "    emb_dat.set_index('pui', inplace=True)\n",
    "\n",
    "    # create embeddings\n",
    "    print('Creating embeddings for all documents...')\n",
    "    for idx, sentence in tqdm(emb_dat.iterrows(), total=len(emb_dat)):\n",
    "        embedded_df.loc[idx] = embed_text(sentence['title'] + ' ' + ' '.join(sentence['keywords'])  + ' ' + sentence['abstract'], model).mean(1).detach().cpu().numpy()\n",
    "\n",
    "    embedded_df.reset_index(names='pui', inplace=True)\n",
    "\n",
    "    return embedded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "601636ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating DataFrame for saving embedding...\n",
      "Creating embeddings for all documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117310/117310 [46:25<00:00, 42.12it/s] \n"
     ]
    }
   ],
   "source": [
    "loc_dict = {\n",
    "        'processed_csv': cc_path('data/processed/canary/articles_cleaned.csv')\n",
    "    }\n",
    "data_loader = DataLoader(loc_dict)\n",
    "processed_df = data_loader.load_processed_csv()\n",
    "label_columns = processed_df.loc[:, ~processed_df.columns.isin(\n",
    "    ['file_name', 'pui', 'title', 'keywords', 'abstract', 'abstract_2', 'authors', 'organization', 'chemicals',\n",
    "     'num_refs', 'date-delivered', 'labels_m', 'labels_a'])]\n",
    "label_columns = label_columns.astype(int)\n",
    "\n",
    "embedding_type = 'scibert'\n",
    "\n",
    "data_for_embedding = processed_df.dropna(subset=['abstract'])\n",
    "data_for_embedding.loc[:, 'labels_m'] = data_for_embedding.loc[:, 'labels_m'].fillna('')\n",
    "# data_for_embedding.loc[:, 'list_label'] = data_for_embedding.loc[:, 'labels_m'].str.split(',')\n",
    "\n",
    "embedding_dim = 768\n",
    "embedded_df = scibert_embedding(data_for_embedding, embedding_dim)\n",
    "\n",
    "\n",
    "embedded_df.to_csv(cc_path(f'data/processed/canary/embeddings_{embedding_type}_paula_finetuned_20230430.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bd32ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bf85f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               pui        d0        d1        d2        d3        d4   \n",
      "0        624531411   0.32247  0.355156 -1.597549 -0.363961 -0.271054  \\\n",
      "1        625340088 -0.352587 -0.111701 -0.667612 -0.085113 -0.369026   \n",
      "2        625805682 -0.656713  1.817787 -0.664207  0.489971 -0.448478   \n",
      "3        626662493  0.040473  0.143472 -0.691856  0.131553  0.252637   \n",
      "4        626822402 -0.414438 -0.311962  0.012679 -0.196783 -0.881608   \n",
      "...            ...       ...       ...       ...       ...       ...   \n",
      "117305  2011621972 -0.667228  0.643817 -0.108718  0.021937  1.114384   \n",
      "117306  2011622024 -0.987457  1.311908 -0.422363  0.249563  0.218531   \n",
      "117307  2011622065   0.32422  0.001175 -0.688419  0.706332  0.896027   \n",
      "117308  2011626864 -0.080151   0.60449 -0.184375  0.027641  -0.37318   \n",
      "117309  2011632199 -1.575297   1.20136   0.02624  0.409932  0.570494   \n",
      "\n",
      "              d5        d6        d7        d8  ...      d758      d759   \n",
      "0        0.18741  0.441012  0.225579  1.303515  ...  0.561036  0.400496  \\\n",
      "1      -0.634241    0.6062 -0.884127 -0.330597  ... -0.228124 -1.139401   \n",
      "2       1.026881   0.90207  0.099757 -0.225848  ...  0.214709  0.357913   \n",
      "3      -0.295331 -0.250432 -1.264591  0.592259  ...  0.628104 -1.774596   \n",
      "4        0.09911  0.190931 -0.253594  0.813877  ...  0.689128  0.340481   \n",
      "...          ...       ...       ...       ...  ...       ...       ...   \n",
      "117305 -0.772772  1.581952  1.257163  0.253562  ...  0.822081 -0.760918   \n",
      "117306 -0.137645 -0.671679  0.017421  0.454559  ...  0.269117 -0.604104   \n",
      "117307 -0.318925  0.382613 -0.615296  0.714484  ...  0.320403 -0.660161   \n",
      "117308  -0.00103 -0.298989 -0.173658  1.194061  ... -0.118799 -0.314717   \n",
      "117309  0.135685 -0.050073 -0.027808  0.178973  ... -0.249387 -0.703759   \n",
      "\n",
      "            d760      d761      d762      d763      d764      d765      d766   \n",
      "0      -0.005806  0.717079  0.158191  1.532672  0.043892  0.472692 -0.435985  \\\n",
      "1       1.084213  0.064648 -0.589766   0.97535  1.263027 -1.156329 -0.303397   \n",
      "2      -0.567658 -0.088406  0.096055  0.849845 -0.612921  0.275656  0.329183   \n",
      "3       0.383434  0.009662  -0.46743  -0.28731  1.154862  0.095788  -0.54795   \n",
      "4      -0.749761  1.122646 -0.553503  -0.14741 -0.468191   0.43015 -0.535591   \n",
      "...          ...       ...       ...       ...       ...       ...       ...   \n",
      "117305 -0.763735  0.634153 -1.099276 -0.717047   1.33831  0.027563  0.523571   \n",
      "117306  0.089399  0.623393  0.335462 -1.366155  1.762151 -1.101769 -0.588092   \n",
      "117307 -0.422733  0.122505  0.192123 -0.019719  0.396624  1.222199 -0.290011   \n",
      "117308 -0.987854  0.439071  0.519308  0.475085 -0.188858 -0.054001  0.846299   \n",
      "117309 -0.634845   0.62179 -0.315445 -0.777813  1.223383 -0.581064 -0.001911   \n",
      "\n",
      "            d767  \n",
      "0       0.525442  \n",
      "1       0.625708  \n",
      "2       0.731084  \n",
      "3       0.213345  \n",
      "4       0.522198  \n",
      "...          ...  \n",
      "117305     1.765  \n",
      "117306  1.559813  \n",
      "117307  0.321318  \n",
      "117308  0.621269  \n",
      "117309  1.635459  \n",
      "\n",
      "[117310 rows x 769 columns]\n"
     ]
    }
   ],
   "source": [
    "print(embedded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07945d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-articleclassifier [Python]",
   "language": "python",
   "name": "conda-env-.conda-articleclassifier-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
