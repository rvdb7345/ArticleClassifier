{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c99fc69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/envs/articleclassifier/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''Code based on: https://www.kaggle.com/code/vpkprasanna/bert-model-with-0-845-accuracy/notebook'''\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "sys.path.append(\"/home/jovyan/20230406_ArticleClassifier/ArticleClassifier\")\n",
    "\n",
    "import src.general.global_variables as gv\n",
    "sys.path.append(\n",
    "    os.path.abspath(os.path.join(os.path.dirname('data_loader.py'), os.path.pardir)))\n",
    "from src.data.data_loader import DataLoader\n",
    "\n",
    "from src.general.utils import cc_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6201f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained SciBERT tokenizer and model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d311f5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Python VERSION: 3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:39:03) \n",
      "[GCC 11.3.0]\n",
      "__pyTorch VERSION: 2.0.0+cu117\n",
      "__CUDA VERSION\n",
      "/bin/bash: nvcc: command not found\n",
      "__CUDNN VERSION: 8500\n",
      "__Number CUDA Devices: 1\n",
      "__Devices\n",
      "Active CUDA Device: GPU 0\n",
      "Available devices  1\n"
     ]
    }
   ],
   "source": [
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA VERSION', )\n",
    "from subprocess import call\n",
    "# call([\"nvcc\", \"--version\"]) does not work\n",
    "! nvcc --version\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "# call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
    "print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
    "print ('Available devices ', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b6bc303",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Bert Model for classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param   bert: a BertModel object\n",
    "        @param   classifier: a torch.nn.Module classifier\n",
    "        @param   freeze_bert (bool): Set `False` to fine_tune the Bert model\n",
    "        \"\"\"\n",
    "        super(BertClassifier,self).__init__()\n",
    "        # Specify hidden size of Bert, hidden size of our classifier, and number of labels\n",
    "        D_in, H,D_out = 768,60,52\n",
    "        \n",
    "#         self.bert = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.bert = BertModel.from_pretrained('scibert_scivocab_uncased')\n",
    "        \n",
    "        self.classifier = torch.nn.Sequential(\n",
    "                            torch.nn.Linear(D_in, H),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(H, D_out))\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "        # Freeze the Bert Model\n",
    "        # Freeze all layers except the last two\n",
    "        for name, param in self.bert.named_parameters():\n",
    "            if 'layer.9' in name or 'layer.10' in name or 'layer.11' in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self,input_ids,attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                           attention_mask = attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:,0,:]\n",
    "        \n",
    "        # Feed input to classifier to compute logits\n",
    "        logit = self.classifier(last_hidden_state_cls)\n",
    "        \n",
    "#         logits = self.sigmoid(logit)\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa7ac870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "    \n",
    "    bert_classifier.to(device)\n",
    "    \n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                     lr=5e-5, #Default learning rate\n",
    "                     eps=1e-8 #Default epsilon value\n",
    "                     )\n",
    "    \n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    \n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                              num_warmup_steps=0, # Default value\n",
    "                                              num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46fc1328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading data...\n"
     ]
    }
   ],
   "source": [
    "# Load the custom dataset\n",
    "print('Start loading data...')\n",
    "loc_dict = {\n",
    "    'processed_csv': cc_path('data/processed/canary/articles_cleaned.csv'),\n",
    "    'abstract_embeddings': cc_path('data/processed/canary/embeddings_fasttext_20230410.csv'),\n",
    "    'scibert_embeddings': cc_path('data/processed/canary/embeddings_scibert_20230413.csv'),\n",
    "    'keyword_network': cc_path('data/processed/canary/keyword_network_weighted.pickle'),\n",
    "    'xml_embeddings': cc_path('data/processed/canary/embeddings_xml.ftr'),\n",
    "    'author_network': cc_path('data/processed/canary/author_network.pickle'), \n",
    "    'label_network': cc_path('data/processed/canary/label_network_weighted.pickle')\n",
    "}\n",
    "data_loader = DataLoader(loc_dict)\n",
    "processed_df = data_loader.load_processed_csv()\n",
    "processed_df.dropna(subset=['abstract'], inplace=True)\n",
    "\n",
    "label_columns = processed_df.loc[:, ~processed_df.columns.isin(\n",
    "    ['file_name', 'title', 'keywords', 'abstract', 'abstract_2', 'authors', 'organization', 'chemicals',\n",
    "     'num_refs', 'date-delivered', 'labels_m', 'labels_a'])]\n",
    "label_columns.loc[:, label_columns.columns.difference(['pui'])] = label_columns.loc[:, \n",
    "    label_columns.columns.difference(['pui'])].astype(int)\n",
    "\n",
    "processed_df['str_keywords'] = processed_df['keywords'].str.replace('[', ' ').str.replace(']', ' ').str.replace(', ', ' ').str.replace(\"'\", '')\n",
    "processed_df['embedding_text'] = processed_df['title'] + processed_df['str_keywords'] + processed_df['abstract']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fee151f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Genetic influence on cognitive development between childhood and adulthood  successful cognitive development childhood adulthood important consequences future mental physical wellbeing occupational financial success delineating genetic influences underlying changes cognitive abilities developmental period provide important insights biological mechanisms govern typical atypical maturation data philadelphia neurodevelopmental cohort pnc large populationbased sample individuals aged 8 21 years old n 6634 empirical relatedness matrix establish heritability general specific cognitive functions determine genetic factors influence cognitive maturation gene age interactions childhood early adulthood neurocognitive measures childhood early adulthood significantly heritable genetic variance general cognitive ability g increased significantly childhood early adulthood finally evidence decay genetic correlation neurocognition childhood adulthood suggesting genetic factors underlie cognition different ages developmental period establishing significant gene age interactions neurocognitive functions childhood early adulthood necessary step identifying genes influence cognitive development genes influence cognition se aberrant cognitive development confers risk psychiatric disorders examination gene age interactions provide important insights etiology'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df['embedding_text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e85bda2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(cc_path(\"data/pui_idx_mapping.json\"), \"r\") as outfile:\n",
    "    node_label_mapping = json.load(outfile)\n",
    "    \n",
    "with open(cc_path(f'data/train_indices.txt')) as f:\n",
    "    train_puis = f.read().splitlines()\n",
    "    train_indices = list(map(node_label_mapping.get, train_puis))\n",
    "with open(cc_path(f'data/val_indices.txt')) as f:\n",
    "    val_puis = f.read().splitlines()\n",
    "    val_indices = list(map(node_label_mapping.get, val_puis))\n",
    "with open(cc_path(f'data/test_indices.txt')) as f:\n",
    "    test_puis = f.read().splitlines()\n",
    "    test_indices = list(map(node_label_mapping.get, test_puis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afcdea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = processed_df.loc[processed_df.pui.isin(train_puis), 'embedding_text'].to_list()\n",
    "train_labels = label_columns.loc[processed_df.pui.isin(train_puis), label_columns.columns.difference(['pui'])].to_numpy().tolist()\n",
    "\n",
    "val_texts = processed_df.loc[processed_df.pui.isin(val_puis), 'embedding_text'].to_list()\n",
    "val_labels = label_columns.loc[processed_df.pui.isin(val_puis), label_columns.columns.difference(['pui'])].to_numpy().tolist()\n",
    "\n",
    "test_texts = processed_df.loc[processed_df.pui.isin(test_puis), 'embedding_text'].to_list()\n",
    "test_labels = label_columns.loc[processed_df.pui.isin(test_puis), label_columns.columns.difference(['pui'])].to_numpy().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9074eed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbe05e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    #for every sentence...\n",
    "    \n",
    "    for sent in tqdm(data):\n",
    "        # 'encode_plus will':\n",
    "        # (1) Tokenize the sentence\n",
    "        # (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        # (3) Truncate/Pad sentence to max length\n",
    "        # (4) Map tokens to their IDs\n",
    "        # (5) Create attention mask\n",
    "        # (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text = sent,   #preprocess sentence\n",
    "            add_special_tokens = True,         #Add `[CLS]` and `[SEP]`\n",
    "            max_length= MAX_LEN  ,             #Max length to truncate/pad\n",
    "            pad_to_max_length = True,          #pad sentence to max length \n",
    "            return_attention_mask= True        #Return attention mask \n",
    "        )\n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "        \n",
    "    #convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    \n",
    "    return input_ids,attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "863c33d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/36055 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/jovyan/.conda/envs/articleclassifier/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 36055/36055 [03:24<00:00, 176.09it/s]\n",
      "100%|██████████| 9014/9014 [00:51<00:00, 176.02it/s]\n",
      "100%|██████████| 11268/11268 [01:03<00:00, 176.88it/s]\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 500\n",
    "\n",
    "# Print sentece 0 and its encoded token ids\n",
    "train_inputs, train_masks = preprocessing_for_bert(train_texts)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "val_inputs, val_masks = preprocessing_for_bert(val_texts)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "test_inputs, test_masks = preprocessing_for_bert(test_texts)\n",
    "test_labels = torch.tensor(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7758aed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "train_data = TensorDataset(train_inputs,train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=64)\n",
    "\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = RandomSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=64)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ba8f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation function\n",
    "def compute_metrics(eval_preds):\n",
    "    labels = eval_preds.label_ids\n",
    "    preds = torch.round(torch.sigmoid(eval_preds.predictions))\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cddd985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify loss function\n",
    "import random\n",
    "import time\n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    max_val_accuracy = 0\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in tqdm(enumerate(train_dataloader)):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels.float())\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20--50000 batches\n",
    "            if (step % 50000 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "            \n",
    "            if val_accuracy > max_val_accuracy:\n",
    "                max_val_accuracy = val_accuracy\n",
    "                torch.save(model.bert, cc_path(f'models/embedders/finetuned_bert_56k_20e_3lay_best_iter.pt'))\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in tqdm(val_dataloader):\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels.float())\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        #preds = torch.argmax(logits, dim=1).flatten()\n",
    "        \n",
    "        # Calculate the accuracy rate\n",
    "        #accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        accuracy = accuracy_thresh(logits.view(-1,52),b_labels.view(-1,52))\n",
    "        \n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "def accuracy_thresh(y_pred, y_true, thresh:float=0.5, sigmoid:bool=True):\n",
    "    \"Compute accuracy when `y_pred` and `y_true` are the same size.\"\n",
    "    if sigmoid: \n",
    "        y_pred = y_pred.sigmoid()\n",
    "    return ((y_pred>thresh)==y_true.byte()).float().mean().item()\n",
    "    #return np.mean(((y_pred>thresh).float()==y_true.float()).float().cpu().numpy(), axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e67aa1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/jovyan/.conda/envs/articleclassifier/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "564it [08:05,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |   563   |   0.241275   |     -      |     -     |  485.32  \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [01:19<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |    -    |   0.241275   |  0.137449  |   0.95    |  565.09  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "564it [08:06,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2    |   563   |   0.126716   |     -      |     -     |  486.31  \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [01:19<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2    |    -    |   0.126716   |  0.117332  |   0.96    |  566.06  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "564it [08:06,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3    |   563   |   0.113026   |     -      |     -     |  486.46  \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [01:19<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3    |    -    |   0.113026   |  0.111063  |   0.96    |  566.19  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "564it [08:06,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   4    |   563   |   0.105982   |     -      |     -     |  486.40  \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [01:19<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   4    |    -    |   0.105982   |  0.108353  |   0.96    |  566.15  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "564it [08:06,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5    |   563   |   0.100614   |     -      |     -     |  486.37  \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [01:19<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5    |    -    |   0.100614   |  0.108051  |   0.96    |  566.11  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "564it [08:06,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6    |   563   |   0.096159   |     -      |     -     |  486.36  \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [01:19<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6    |    -    |   0.096159   |  0.106993  |   0.96    |  566.11  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "564it [08:06,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   7    |   563   |   0.091988   |     -      |     -     |  486.36  \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [01:19<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   7    |    -    |   0.091988   |  0.106759  |   0.96    |  566.08  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "564it [08:06,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   8    |   563   |   0.088012   |     -      |     -     |  486.19  \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [01:19<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   8    |    -    |   0.088012   |  0.108356  |   0.96    |  565.95  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "164it [02:21,  1.16it/s]"
     ]
    }
   ],
   "source": [
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=30)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=30, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9d7f58b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bert_classifier.bert, cc_path(f'models/embedders/finetuned_bert_56k_20e_3lay.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ee7859",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-articleclassifier [Python]",
   "language": "python",
   "name": "conda-env-.conda-articleclassifier-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
