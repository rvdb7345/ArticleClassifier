{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21be2b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Code based on: https://www.kaggle.com/code/vpkprasanna/bert-model-with-0-845-accuracy/notebook'''\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "sys.path.append(\"/home/jovyan/20230406_ArticleClassifier/ArticleClassifier\")\n",
    "\n",
    "import src.general.global_variables as gv\n",
    "sys.path.append(\n",
    "    os.path.abspath(os.path.join(os.path.dirname('data_loader.py'), os.path.pardir)))\n",
    "from src.data.data_loader import DataLoader\n",
    "\n",
    "from src.general.utils import cc_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "056a0dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained SciBERT tokenizer and model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "# tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "024d8e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Python VERSION: 3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:39:03) \n",
      "[GCC 11.3.0]\n",
      "__pyTorch VERSION: 2.0.0+cu117\n",
      "__CUDA VERSION\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Tue_May__3_18:49:52_PDT_2022\n",
      "Cuda compilation tools, release 11.7, V11.7.64\n",
      "Build cuda_11.7.r11.7/compiler.31294372_0\n",
      "__CUDNN VERSION: 8500\n",
      "__Number CUDA Devices: 1\n",
      "__Devices\n",
      "Active CUDA Device: GPU 0\n",
      "Available devices  1\n"
     ]
    }
   ],
   "source": [
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA VERSION', )\n",
    "from subprocess import call\n",
    "# call([\"nvcc\", \"--version\"]) does not work\n",
    "! nvcc --version\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "# call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
    "print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
    "print ('Available devices ', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1f79fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Bert Model for classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param   bert: a BertModel object\n",
    "        @param   classifier: a torch.nn.Module classifier\n",
    "        @param   freeze_bert (bool): Set `False` to fine_tune the Bert model\n",
    "        \"\"\"\n",
    "        super(BertClassifier,self).__init__()\n",
    "        # Specify hidden size of Bert, hidden size of our classifier, and number of labels\n",
    "        D_in, H,D_out = 768,60,7\n",
    "        \n",
    "#         self.bert = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.bert = BertModel.from_pretrained('scibert_scivocab_uncased')\n",
    "#         self.bert = torch.load(cc_path(f'models/baselines/paula_finetuned_bert_56k_10e_tka.pt')).base_model\n",
    "#         self.bert = BertModel.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\n",
    "\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "                            torch.nn.Linear(D_in, H),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(H, D_out))\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "        # Freeze the Bert Model\n",
    "        # Freeze all layers except the last two\n",
    "        for name, param in self.bert.named_parameters():\n",
    "            if 'layer.9' in name or 'layer.10' in name or 'layer.11' in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self,input_ids,attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                           attention_mask = attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:,0,:]\n",
    "#         last_hidden_state_cls = outputs[0]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logit = self.classifier(last_hidden_state_cls)\n",
    "        \n",
    "#         logits = self.sigmoid(logit)\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a60809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "    \n",
    "    bert_classifier.to(device)\n",
    "    \n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                     lr=5e-5, #Default learning rate\n",
    "                     eps=1e-8 #Default epsilon value\n",
    "                     )\n",
    "    \n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    \n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                              num_warmup_steps=0, # Default value\n",
    "                                              num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dca7ef0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bert_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlist\u001b[39m(\u001b[43mbert_classifier\u001b[49m\u001b[38;5;241m.\u001b[39mbert\u001b[38;5;241m.\u001b[39mnamed_parameters())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bert_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "list(bert_classifier.bert.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8da4431d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading data...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataLoader' object has no attribute 'load_processed_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m loc_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_csv\u001b[39m\u001b[38;5;124m'\u001b[39m: cc_path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/processed/canary/articles_cleaned.csv\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstract_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: cc_path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/processed/canary/embeddings_fasttext_20230410.csv\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_network\u001b[39m\u001b[38;5;124m'\u001b[39m: cc_path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/processed/canary/label_network_weighted.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m }\n\u001b[1;32m     12\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m DataLoader(loc_dict)\n\u001b[0;32m---> 13\u001b[0m processed_df \u001b[38;5;241m=\u001b[39m \u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_processed_csv\u001b[49m()\n\u001b[1;32m     14\u001b[0m processed_df\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m label_columns \u001b[38;5;241m=\u001b[39m processed_df\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;241m~\u001b[39mprocessed_df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39misin(\n\u001b[1;32m     17\u001b[0m     [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstract_2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthors\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morganization\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchemicals\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     18\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_refs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate-delivered\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels_m\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels_a\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'load_processed_csv'"
     ]
    }
   ],
   "source": [
    "# Load the custom dataset\n",
    "print('Start loading data...')\n",
    "loc_dict = {\n",
    "    'processed_csv': cc_path('data/processed/canary/articles_cleaned.csv'),\n",
    "    'abstract_embeddings': cc_path('data/processed/canary/embeddings_fasttext_20230410.csv'),\n",
    "    'scibert_embeddings': cc_path('data/processed/canary/embeddings_scibert_20230413.csv'),\n",
    "    'keyword_network': cc_path('data/processed/canary/keyword_network_weighted.pickle'),\n",
    "    'xml_embeddings': cc_path('data/processed/canary/embeddings_xml.ftr'),\n",
    "    'author_network': cc_path('data/processed/canary/author_network.pickle'), \n",
    "    'label_network': cc_path('data/processed/canary/label_network_weighted.pickle')\n",
    "}\n",
    "data_loader = DataLoader(loc_dict)\n",
    "processed_df = data_loader.load_processed_csv()\n",
    "processed_df.dropna(subset=['abstract'], inplace=True)\n",
    "\n",
    "label_columns = processed_df.loc[:, ~processed_df.columns.isin(\n",
    "    ['file_name', 'title', 'keywords', 'abstract', 'abstract_2', 'authors', 'organization', 'chemicals',\n",
    "     'num_refs', 'date-delivered', 'labels_m', 'labels_a'])]\n",
    "label_columns.loc[:, label_columns.columns.difference(['pui'])] = label_columns.loc[:, \n",
    "    label_columns.columns.difference(['pui'])].astype(int)\n",
    "\n",
    "processed_df['str_keywords'] = processed_df['keywords'].str.replace('[', ' ').str.replace(']', ' ').str.replace(', ', ' ').str.replace(\"'\", '')\n",
    "processed_df['embedding_text'] = processed_df['title'] + processed_df['str_keywords'] + processed_df['abstract']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af346e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading data...\n"
     ]
    }
   ],
   "source": [
    "# Load the custom dataset\n",
    "print('Start loading data...')\n",
    "loc_dict = {\n",
    "    'processed_csv': cc_path('data/processed/litcovid/litcovid_articles_cleaned_20230529.csv'),\n",
    "    'scibert_embeddings': cc_path('data/processed/litcovid/litcovid_embeddings_scibert_finetuned_20230425.csv'),\n",
    "    'keyword_network': cc_path('data/processed/litcovid/litcovid_keyword_network_weighted.pickle'),\n",
    "    'xml_embeddings': cc_path('data/processed/litcovid/litcovid_embeddings_xml_20230518_68.ftr'),\n",
    "    'label_network': cc_path('data/processed/litcovid/litcovid_label_network_weighted.pickle')\n",
    "}\n",
    "data_loader = DataLoader(loc_dict)\n",
    "processed_df = data_loader.load_processed_csv()\n",
    "processed_df.dropna(subset=['abstract'], inplace=True)\n",
    "\n",
    "label_columns = processed_df.loc[:, ~processed_df.columns.isin(\n",
    "    ['file_name', 'title', 'keywords', 'abstract', 'abstract_2', 'authors', 'organization', 'chemicals',\n",
    "     'num_refs', 'date-delivered', 'labels_m', 'labels_a', 'journal', 'pub_type', 'doi', 'label', 'label_m', 'list_label'])]\n",
    "label_columns.loc[:, label_columns.columns.difference(['pui'])] = label_columns.loc[:, \n",
    "    label_columns.columns.difference(['pui'])].astype(int)\n",
    "\n",
    "processed_df['str_keywords'] = processed_df['keywords'].str.replace('[', ' ').str.replace(']', ' ').str.replace(', ', ' ').str.replace(\"'\", '')\n",
    "processed_df['embedding_text'] = processed_df['title'] + \" \" + processed_df['journal'] + \" \" + processed_df['pub_type'].str.replace(';', ' ') + \" \" + processed_df['str_keywords'] + processed_df['abstract']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c5d6ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Potential role for tissue factor in the pathogenesis of hypercoagulability associated with in COVID-19. J Thromb Thrombolysis Journal Article Review  il-6 tnf-alpha thrombosis tissue factor in december 2019 a new and highly contagious infectious disease emerged in wuhan china the etiologic agent was identified as a novel coronavirus now known as severe acute syndrome coronavirus2 sarscov2 recent research has revealed that virus entry takes place upon the union of the virus s surface protein with the type i transmembrane metallocarboxypeptidase angiotensin converting enzyme 2 ace2 identified on epithelial cells of the host respiratory tract virus triggers the synthesis and release of proinflammatory cytokines including il6 and tnfalpha and also promotes downregulation of ace2 which promotes a concomitant increase in levels of angiotensin ii atii both tnfalpha and atii have been implicated in promoting overexpression of tissue factor tf in platelets and macrophages additionally the generation of antiphospholipid antibodies associated with covid19 may also promote an increase in tf tf may be a critical mediator associated with the development of thrombotic phenomena in covid19 and should be a target for future study'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df['embedding_text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1434e77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b3be3e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jovyan/20230406_ArticleClassifier/ArticleClassifier/data/pui_idx_mapping.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcc_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/pui_idx_mapping.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m outfile:\n\u001b[1;32m      2\u001b[0m     node_label_mapping \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(outfile)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(cc_path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/train_indices.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jovyan/20230406_ArticleClassifier/ArticleClassifier/data/pui_idx_mapping.json'"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(cc_path(\"data/pui_idx_mapping.json\"), \"r\") as outfile:\n",
    "    node_label_mapping = json.load(outfile)\n",
    "    \n",
    "with open(cc_path(f'data/train_indices.txt')) as f:\n",
    "    train_puis = f.read().splitlines()\n",
    "    train_indices = list(map(node_label_mapping.get, train_puis))\n",
    "with open(cc_path(f'data/val_indices.txt')) as f:\n",
    "    val_puis = f.read().splitlines()\n",
    "    val_indices = list(map(node_label_mapping.get, val_puis))\n",
    "with open(cc_path(f'data/test_indices.txt')) as f:\n",
    "    test_puis = f.read().splitlines()\n",
    "    test_indices = list(map(node_label_mapping.get, test_puis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60e48104",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cc_path(f'data/litcovid_train_indices.txt')) as f:\n",
    "    train_puis = f.read().splitlines()\n",
    "with open(cc_path(f'data/litcovid_val_indices.txt')) as f:\n",
    "    val_puis = f.read().splitlines()\n",
    "with open(cc_path(f'data/litcovid_test_indices.txt')) as f:\n",
    "    test_puis = f.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f338aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = processed_df.loc[processed_df.pui.isin(train_puis), 'embedding_text'].to_list()\n",
    "train_labels = label_columns.loc[processed_df.pui.isin(train_puis), label_columns.columns.difference(['pui'])].to_numpy().tolist()\n",
    "\n",
    "val_texts = processed_df.loc[processed_df.pui.isin(val_puis), 'embedding_text'].to_list()\n",
    "val_labels = label_columns.loc[processed_df.pui.isin(val_puis), label_columns.columns.difference(['pui'])].to_numpy().tolist()\n",
    "\n",
    "test_texts = processed_df.loc[processed_df.pui.isin(test_puis), 'embedding_text'].to_list()\n",
    "test_labels = label_columns.loc[processed_df.pui.isin(test_puis), label_columns.columns.difference(['pui'])].to_numpy().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3d52f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "833dd822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    #for every sentence...\n",
    "    \n",
    "    for sent in tqdm(data):\n",
    "        # 'encode_plus will':\n",
    "        # (1) Tokenize the sentence\n",
    "        # (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        # (3) Truncate/Pad sentence to max length\n",
    "        # (4) Map tokens to their IDs\n",
    "        # (5) Create attention mask\n",
    "        # (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text = sent,   #preprocess sentence\n",
    "            add_special_tokens = True,         #Add `[CLS]` and `[SEP]`\n",
    "            max_length= MAX_LEN  ,             #Max length to truncate/pad\n",
    "            pad_to_max_length = True,          #pad sentence to max length \n",
    "            return_attention_mask= True        #Return attention mask \n",
    "        )\n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        \n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "        \n",
    "    #convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    \n",
    "    return input_ids,attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5bd7e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24947 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/jovyan/.conda/envs/articleclassifier/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 24947/24947 [02:36<00:00, 159.43it/s]\n",
      "100%|██████████| 6236/6236 [00:39<00:00, 158.11it/s]\n",
      "100%|██████████| 2489/2489 [00:16<00:00, 150.31it/s]\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 512\n",
    "\n",
    "# Print sentece 0 and its encoded token ids\n",
    "train_inputs, train_masks = preprocessing_for_bert(train_texts)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "val_inputs, val_masks = preprocessing_for_bert(val_texts)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "test_inputs, test_masks = preprocessing_for_bert(test_texts)\n",
    "test_labels = torch.tensor(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5452a1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "train_data = TensorDataset(train_inputs,train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
    "\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = RandomSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=32)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d8f3a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation function\n",
    "def compute_metrics(eval_preds):\n",
    "    labels = eval_preds.label_ids\n",
    "    preds = torch.round(torch.sigmoid(eval_preds.predictions))\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1f3909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify loss function\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "from sklearn.metrics import f1_score\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    max_val_accuracy = 0\n",
    "    not_improved = 0\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in tqdm(enumerate(train_dataloader)):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels.float())\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20--50000 batches\n",
    "            if (step % 50000 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy, _ = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.5f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "            \n",
    "            if val_accuracy > max_val_accuracy:\n",
    "                max_val_accuracy = val_accuracy\n",
    "                best_model = copy.deepcopy(model)\n",
    "                torch.save(model.bert, cc_path(f'models/embedders/litcovid_pretrained_best_iter_meta_stopwords.pt'))\n",
    "                not_improved = 0\n",
    "            else:\n",
    "                not_improved += 1\n",
    "            \n",
    "            if not_improved == 5:\n",
    "                break\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    return best_model\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    list_val_f1_micro = []\n",
    "    list_val_f1_macro = []\n",
    "    val_loss = []\n",
    "\n",
    "    predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in tqdm(val_dataloader):\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels.float())\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Append batch predictions to the list\n",
    "        predictions.append(logits)\n",
    "        all_labels.append(b_labels)\n",
    "\n",
    "    # Combine predictions for all batches into a single tensor\n",
    "    predictions = torch.cat(predictions, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    # Calculate f1 scores\n",
    "    f1_micro, f1_macro = accuracy_thresh(predictions.view(-1, 7), all_labels.view(-1, 7))\n",
    "\n",
    "    # Compute the average loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    # Calculate the average f1 scores\n",
    "    val_f1_micro = f1_micro.mean().item()\n",
    "    val_f1_macro = f1_macro.mean().item()\n",
    "\n",
    "    return val_loss, val_f1_micro, val_f1_macro\n",
    "\n",
    "\n",
    "def accuracy_thresh(y_pred, y_true, thresh:float=0.5, sigmoid:bool=True):\n",
    "    \"Compute accuracy when `y_pred` and `y_true` are the same size.\"\n",
    "    if sigmoid: \n",
    "        y_pred = y_pred.sigmoid()\n",
    "\n",
    "    y_pred[(y_pred>thresh)] = 1\n",
    "    y_pred[(y_pred<thresh)] = 0\n",
    "    return f1_score(y_true.byte().cpu(), y_pred.cpu(), average='micro'), f1_score(y_true.byte().cpu(), y_pred.cpu(), average='macro')\n",
    "    #return np.mean(((y_pred>thresh).float()==y_true.float()).float().cpu().numpy(), axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fadc7f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/jovyan/.conda/envs/articleclassifier/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "780it [05:44,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |   779   |   0.156363   |     -      |     -     |  344.76  \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:56<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |    -    |   0.156363   |  0.108553  |  0.89542  |  401.58  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "780it [05:45,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2    |   779   |   0.098123   |     -      |     -     |  345.16  \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:56<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2    |    -    |   0.098123   |  0.101684  |  0.89923  |  401.97  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "780it [05:45,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3    |   779   |   0.084347   |     -      |     -     |  345.39  \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:56<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3    |    -    |   0.084347   |  0.100348  |  0.90190  |  402.21  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "780it [05:46,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   4    |   779   |   0.072762   |     -      |     -     |  346.63  \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:56<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   4    |    -    |   0.072762   |  0.102647  |  0.90229  |  403.55  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "780it [05:45,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5    |   779   |   0.059894   |     -      |     -     |  345.75  \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:56<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5    |    -    |   0.059894   |  0.104822  |  0.89977  |  402.61  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "780it [05:46,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6    |   779   |   0.049583   |     -      |     -     |  346.51  \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:56<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6    |    -    |   0.049583   |  0.112176  |  0.90181  |  403.33  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "780it [05:46,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   7    |   779   |   0.039246   |     -      |     -     |  346.14  \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:56<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   7    |    -    |   0.039246   |  0.117805  |  0.90201  |  402.95  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "780it [05:45,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   8    |   779   |   0.030517   |     -      |     -     |  345.74  \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:56<00:00,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   8    |    -    |   0.030517   |  0.126527  |  0.89854  |  402.72  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "780it [05:45,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   9    |   779   |   0.023950   |     -      |     -     |  345.98  \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:56<00:00,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   9    |    -    |   0.023950   |  0.138084  |  0.89840  |  402.80  \n",
      "----------------------------------------------------------------------\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=15)\n",
    "best_model = train(bert_classifier, train_dataloader, val_dataloader, epochs=15, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "249a5c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del bert_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15159c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model.bert, cc_path(f'models/embedders/litcovid_finetuned_bert_20e_3lay_meta.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e185e39",
   "metadata": {},
   "source": [
    "# evaluate BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c4fb01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:22<00:00,  3.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.09495743330663596, 0.909576712136127, 0.8659726267674196)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " evaluate(best_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af502ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-articleclassifier [Python]",
   "language": "python",
   "name": "conda-env-.conda-articleclassifier-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
