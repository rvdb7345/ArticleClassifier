{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8cafe45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vocab building: : 456009it [01:04, 7053.74it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import math\n",
    "import numpy as np\n",
    "import timeit\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "sys.path.append(\"/home/jovyan/20230406_ArticleClassifier/ArticleClassifier\")\n",
    "\n",
    "import src.general.global_variables as gv\n",
    "from src.general.utils import cc_path\n",
    "\n",
    "sys.path.append(gv.PROJECT_PATH)\n",
    "\n",
    "\n",
    "sys.path.append(\n",
    "    os.path.abspath(os.path.join(os.path.dirname('data_loader.py'), os.path.pardir)))\n",
    "from src.data.data_loader import DataLoader\n",
    "\n",
    "# def load_data(data_path, max_length, vocab_size, batch_size=64):\n",
    "#     X_trn, Y_trn, X_tst, Y_tst, vocabulary, vocabulary_inv = data_helpers.load_data(data_path, max_length=max_length,\n",
    "#                                                                                     vocab_size=vocab_size)\n",
    "#     Y_trn = Y_trn[0:].toarray()\n",
    "#     Y_trn = np.insert(Y_trn, 101, 0, axis=1)\n",
    "#     Y_trn = np.insert(Y_trn, 102, 0, axis=1)\n",
    "#     Y_tst = Y_tst[0:].toarray()\n",
    "#\n",
    "#     train_data = data_utils.TensorDataset(torch.from_numpy(X_trn).type(torch.LongTensor),\n",
    "#                                           torch.from_numpy(Y_trn).type(torch.LongTensor))\n",
    "#     test_data = data_utils.TensorDataset(torch.from_numpy(X_tst).type(torch.LongTensor),\n",
    "#                                          torch.from_numpy(Y_tst).type(torch.LongTensor))\n",
    "#     train_loader = data_utils.DataLoader(train_data, batch_size, drop_last=False, shuffle=True)\n",
    "#     test_loader = data_utils.DataLoader(test_data, batch_size, drop_last=False)\n",
    "#     return train_loader, test_loader, vocabulary, X_tst, Y_tst, X_trn, Y_trn\n",
    "\n",
    "def precision_k(pred, label, k=[1, 3, 5]):\n",
    "    batch_size = pred.shape[0]\n",
    "\n",
    "    precision = []\n",
    "    for _k in k:\n",
    "        p = 0\n",
    "        for i in range(batch_size):\n",
    "            p += label[i, pred[i, :_k]].mean()\n",
    "        precision.append(p * 100 / batch_size)\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "def ndcg_k(pred, label, k=[1, 3, 5]):\n",
    "    batch_size = pred.shape[0]\n",
    "\n",
    "    ndcg = []\n",
    "    for _k in k:\n",
    "        score = 0\n",
    "        rank = np.log2(np.arange(2, 2 + _k))\n",
    "        for i in range(batch_size):\n",
    "            l = label[i, pred[i, :_k]]\n",
    "            n = l.sum()\n",
    "            if (n == 0):\n",
    "                continue\n",
    "\n",
    "            dcg = (l / rank).sum()\n",
    "            label_count = label[i].sum()\n",
    "            norm = 1 / np.log2(np.arange(2, 2 + np.min((_k, label_count))))\n",
    "            norm = norm.sum()\n",
    "            score += dcg / norm\n",
    "\n",
    "        ndcg.append(score * 100 / batch_size)\n",
    "\n",
    "    return ndcg\n",
    "\n",
    "\n",
    "# input data_path\n",
    "# data_path = '/data/rcv1_raw_text.p'\n",
    "sequence_length = 500\n",
    "batch_size = 64\n",
    "\n",
    "def load_glove_embeddings(path, embedding_dim):\n",
    "    \"\"\"Loading the glove embeddings\"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        embeddings = np.zeros((len(lines)+1, embedding_dim))\n",
    "        word_idx_dict = {}\n",
    "        for index, line in tqdm(enumerate(lines), desc='vocab building: '):\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            word_idx_dict[word] = index + 1\n",
    "            if index:\n",
    "                vector = np.array(values[1:], dtype='float32')\n",
    "                if vector.shape[-1] != embedding_dim:\n",
    "                    raise Exception('Dimension not matching.')\n",
    "                embeddings[index] = vector\n",
    "\n",
    "    return torch.from_numpy(embeddings).float(), word_idx_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# load glove\n",
    "pretrain = 'glove'\n",
    "embedding_dim = 256\n",
    "# input word2vec file path\n",
    "file_path = cc_path(f'data/processed/canary/word_embeddings_{pretrain}_2023-04-11.csv')\n",
    "embedding_weights, word_idx_dict = load_glove_embeddings(file_path, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f97e2fc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b796b468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/envs/articleclassifier/lib/python3.9/site-packages/pandas/core/indexing.py:1715: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, v)\n"
     ]
    }
   ],
   "source": [
    "print('-' * 50)\n",
    "print('Loading data...');\n",
    "start_time = timeit.default_timer()\n",
    "# load all the data\n",
    "loc_dict = {\n",
    "    'processed_csv': cc_path('data/processed/canary/articles_cleaned.csv'),\n",
    "    'abstract_embeddings': cc_path('data/processed/canary/embeddings_fasttext_20230410.csv'),\n",
    "    'keyword_network': cc_path('data/processed/canary/keyword_network_weighted.pickle'),\n",
    "    'author_network': cc_path('data/processed/canary/author_network.pickle')\n",
    "}\n",
    "data_loader = DataLoader(loc_dict)\n",
    "processed_df = data_loader.load_processed_csv()\n",
    "\n",
    "processed_df['pui'] = processed_df['pui'].astype(str)\n",
    "\n",
    "\n",
    "label_columns = processed_df.loc[:, ~processed_df.columns.isin(\n",
    "    ['file_name', 'title', 'keywords', 'abstract', 'abstract_2', 'authors', 'organization', 'chemicals',\n",
    "     'num_refs', 'date-delivered', 'labels_m', 'labels_a'])]\n",
    "label_columns.loc[:, label_columns.columns.difference(['pui'])] = label_columns.loc[\n",
    "                                                                  :, label_columns.columns.difference(['pui'])].astype(str)\n",
    "\n",
    "with open(cc_path(f'data/train_indices.txt')) as f:\n",
    "    train_puis = f.read().splitlines()\n",
    "with open(cc_path(f'data/val_indices.txt')) as f:\n",
    "    val_puis = f.read().splitlines()\n",
    "with open(cc_path(f'data/test_indices.txt')) as f:\n",
    "    test_puis = f.read().splitlines()\n",
    "        \n",
    "processed_df['str_keywords'] = processed_df['keywords'].str.replace('[', ' ').str.replace(']', ' ').str.replace(', ', ' ').str.replace(\"'\", '')\n",
    "processed_df['embedding_text'] = processed_df['title'] + processed_df['str_keywords'] + processed_df['abstract']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ec60bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = processed_df[processed_df.pui.isin(train_puis)]\n",
    "val_set = processed_df[processed_df.pui.isin(val_puis)]\n",
    "test_set = processed_df[processed_df.pui.isin(test_puis)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ccdc6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = label_columns.loc[processed_df.pui.isin(train_puis), label_columns.columns.difference(['pui'])].to_numpy(dtype=np.int8)\n",
    "val_labels = label_columns.loc[processed_df.pui.isin(val_puis), label_columns.columns.difference(['pui'])].to_numpy(dtype=np.int8)\n",
    "test_labels = label_columns.loc[processed_df.pui.isin(test_puis), label_columns.columns.difference(['pui'])].to_numpy(dtype=np.int8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4c82208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0]], dtype=int8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfbf4324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1015/504266889.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_set['abstract'] = train_set['abstract'].apply(lambda x: [int(word_idx_dict[word]) for word in x.split(' ')])\n",
      "/tmp/ipykernel_1015/504266889.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_set['abstract'] = val_set['abstract'].apply(lambda x: [int(word_idx_dict[word]) for word in x.split(' ')])\n",
      "/tmp/ipykernel_1015/504266889.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_set['abstract'] = test_set['abstract'].apply(lambda x: [int(word_idx_dict[word]) for word in x.split(' ')])\n"
     ]
    }
   ],
   "source": [
    "# processed_df['embedding_text'] = processed_df['embedding_text'].apply(lambda x: [int(word_idx_dict[word]) for word in x.split(' ')])\n",
    "train_set['abstract'] = train_set['abstract'].apply(lambda x: [int(word_idx_dict[word]) for word in x.split(' ')])\n",
    "val_set['abstract'] = val_set['abstract'].apply(lambda x: [int(word_idx_dict[word]) for word in x.split(' ')])\n",
    "test_set['abstract'] = test_set['abstract'].apply(lambda x: [int(word_idx_dict[word]) for word in x.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55573bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_set['abstract'].apply(len).hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e207cc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1015/3275558676.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_set['abstract'] = train_set['abstract'].apply(lambda x: x + [0] * (set_width - len(x)) if len(x) <= set_width else x[:set_width])\n",
      "/tmp/ipykernel_1015/3275558676.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_set['abstract'] = val_set['abstract'].apply(lambda x: x + [0] * (set_width - len(x)) if len(x) <= set_width else x[:set_width])\n",
      "/tmp/ipykernel_1015/3275558676.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_set['abstract'] = test_set['abstract'].apply(lambda x: x + [0] * (set_width - len(x)) if len(x) <= set_width else x[:set_width])\n"
     ]
    }
   ],
   "source": [
    "set_width = 200\n",
    "train_set['abstract'] = train_set['abstract'].apply(lambda x: x + [0] * (set_width - len(x)) if len(x) <= set_width else x[:set_width])\n",
    "val_set['abstract'] = val_set['abstract'].apply(lambda x: x + [0] * (set_width - len(x)) if len(x) <= set_width else x[:set_width])\n",
    "test_set['abstract'] = test_set['abstract'].apply(lambda x: x + [0] * (set_width - len(x)) if len(x) <= set_width else x[:set_width])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e9f88cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process time 6181.528 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "train_data = data_utils.TensorDataset(torch.from_numpy(np.array(train_set['abstract'].to_list(), dtype=int)).type(torch.LongTensor).to(device),\n",
    "                                      torch.from_numpy(train_labels).type(torch.LongTensor).to(device))\n",
    "val_data = data_utils.TensorDataset(torch.from_numpy(np.array(val_set['abstract'].to_list(), dtype=int)).type(torch.LongTensor).to(device),\n",
    "                                     torch.from_numpy(val_labels).type(torch.LongTensor).to(device))\n",
    "test_data = data_utils.TensorDataset(torch.from_numpy(np.array(test_set['abstract'].to_list(), dtype=int)).type(torch.LongTensor).to(device),\n",
    "                                     torch.from_numpy(test_labels).type(torch.LongTensor).to(device))\n",
    "train_loader = data_utils.DataLoader(train_data, batch_size, drop_last=False, shuffle=True)\n",
    "val_loader = data_utils.DataLoader(val_data, batch_size, drop_last=False)\n",
    "test_loader = data_utils.DataLoader(test_data, batch_size, drop_last=False)\n",
    "\n",
    "\n",
    "# train_loader, test_loader, vocabulary, X_tst, Y_tst, X_trn, Y_trn = load_data(data_path, sequence_length, vocab_size,\n",
    "#                                                                               batch_size)\n",
    "print('Process time %.3f (secs)\\n' % (timeit.default_timer() - start_time))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vocab_size = embedding_weights.size(0)\n",
    "\n",
    "# create Network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "925e4851",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBasicModule\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m(BasicModule, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class BasicModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicModule, self).__init__()\n",
    "        self.model_name = str(type(self))\n",
    "\n",
    "    def load(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "\n",
    "    def save(self, path=None):\n",
    "        if path is None:\n",
    "            raise ValueError('Please specify the saving road!!!')\n",
    "        torch.save(self.state_dict(), path)\n",
    "        return path\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "def get_embedding_layer(embedding_weights):\n",
    "    word_embeddings = nn.Embedding(num_embeddings=embedding_weights.size(0), embedding_dim=embedding_weights.size(1))\n",
    "    word_embeddings.weight.data.copy_(embedding_weights)\n",
    "    word_embeddings.weight.requires_grad = False  # not train\n",
    "    return word_embeddings\n",
    "\n",
    "\n",
    "class Hybrid_XML(BasicModule):\n",
    "    def __init__(self, num_labels=3714, vocab_size=30001, embedding_size=300, embedding_weights=None,\n",
    "                 max_seq=300, hidden_size=256, d_a=256, label_emb=None):\n",
    "        super(Hybrid_XML, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_labels = num_labels\n",
    "        self.max_seq = max_seq\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        if embedding_weights is None:\n",
    "            self.word_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        else:\n",
    "            self.word_embeddings = get_embedding_layer(embedding_weights)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_size, hidden_size=self.hidden_size, num_layers=1,\n",
    "                            batch_first=True, bidirectional=True)\n",
    "\n",
    "        # interaction-attention layer\n",
    "        self.key_layer = torch.nn.Linear(2 * self.hidden_size, self.hidden_size)\n",
    "        self.query_layer = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "        # self-attn layer\n",
    "        self.linear_first = torch.nn.Linear(2 * self.hidden_size, d_a)\n",
    "        self.linear_second = torch.nn.Linear(d_a, self.num_labels)\n",
    "\n",
    "        # weight adaptive layer\n",
    "        self.linear_weight1 = torch.nn.Linear(2 * self.hidden_size, 1)\n",
    "        self.linear_weight2 = torch.nn.Linear(2 * self.hidden_size, 1)\n",
    "\n",
    "        # shared for all attention component\n",
    "        self.linear_final = torch.nn.Linear(2 * self.hidden_size, self.hidden_size)\n",
    "        self.output_layer = torch.nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "        label_embedding = torch.FloatTensor(self.num_labels, self.hidden_size)\n",
    "        if label_emb is None:\n",
    "            nn.init.xavier_normal_(label_embedding)\n",
    "        else:\n",
    "            label_embedding.copy_(label_emb)\n",
    "        self.label_embedding = nn.Parameter(label_embedding, requires_grad=False)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        if torch.cuda.is_available():\n",
    "            return (\n",
    "            torch.zeros(2, batch_size, self.hidden_size).cuda(), torch.zeros(2, batch_size, self.hidden_size).cuda())\n",
    "        else:\n",
    "            return (torch.zeros(2, batch_size, self.hidden_size), torch.zeros(2, batch_size, self.hidden_size))\n",
    "\n",
    "    def forward(self, x, embedding_generation=False):\n",
    "\n",
    "        emb = self.word_embeddings(x)\n",
    "\n",
    "        hidden_state = self.init_hidden(emb.size(0))\n",
    "        output, hidden_state = self.lstm(emb, hidden_state)  # [batch,seq,2*hidden]\n",
    "        print(output.size())\n",
    "\n",
    "        # get attn_key\n",
    "        attn_key = self.key_layer(output)  # [batch,seq,hidden]\n",
    "        attn_key = attn_key.transpose(1, 2)  # [batch,hidden,seq]\n",
    "        # get attn_query\n",
    "        label_emb = self.label_embedding.expand(\n",
    "            (attn_key.size(0), self.label_embedding.size(0), self.label_embedding.size(1)))  # [batch,L,label_emb]\n",
    "        label_emb = self.query_layer(label_emb)  # [batch,L,label_emb]\n",
    "\n",
    "        # attention\n",
    "        similarity = torch.bmm(label_emb, attn_key)  # [batch,L,seq]\n",
    "        similarity = F.softmax(similarity, dim=2)\n",
    "        out1 = torch.bmm(similarity, output)  # [batch,L,label_emb]\n",
    "\n",
    "        # self-attn output\n",
    "        self_attn = torch.tanh(self.linear_first(output))  # [batch,seq,d_a]\n",
    "        self_attn = self.linear_second(self_attn)  # [batch,seq,L]\n",
    "        self_attn = F.softmax(self_attn, dim=1)\n",
    "        self_attn = self_attn.transpose(1, 2)  # [batch,L,seq]\n",
    "        out2 = torch.bmm(self_attn, output)  # [batch,L,hidden]\n",
    "\n",
    "        factor1 = torch.sigmoid(self.linear_weight1(out1))\n",
    "        factor2 = torch.sigmoid(self.linear_weight2(out2))\n",
    "        factor1 = factor1 / (factor1 + factor2)\n",
    "        factor2 = 1 - factor1\n",
    "\n",
    "        out = factor1 * out1 + factor2 * out2\n",
    "        \n",
    "        if embedding_generation:\n",
    "            return out\n",
    "        \n",
    "        out = F.relu(self.linear_final(out))\n",
    "        out = torch.sigmoid(self.output_layer(out).squeeze(-1))  # [batch,L]\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5f6d495",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_emb = np.zeros((52, 52))\n",
    "label_index_mapping = {}\n",
    "with open(cc_path(f'notebooks/label_embedding_test.txt')) as f:\n",
    "    for index, i in enumerate(f.readlines()):\n",
    "        if index == 0:\n",
    "            continue\n",
    "        i = i.rstrip('\\n')\n",
    "        n = i.split(',')[0]\n",
    "        content = i.split(',')[1].split(' ')\n",
    "        label_index_mapping[index-1] = n\n",
    "        label_emb[index-1] = [float(value) for value in content]\n",
    "\n",
    "\n",
    "\n",
    "label_emb = torch.from_numpy(label_emb).float()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a831362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b046901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Hybrid_XML(num_labels=52, vocab_size=len(word_idx_dict), embedding_size=256, embedding_weights=embedding_weights,\n",
    "                   max_seq=200, hidden_size=52, d_a=256, label_emb=label_emb).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b67d589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid_XML(\n",
      "  (word_embeddings): Embedding(456010, 256)\n",
      "  (lstm): LSTM(256, 52, batch_first=True, bidirectional=True)\n",
      "  (key_layer): Linear(in_features=104, out_features=52, bias=True)\n",
      "  (query_layer): Linear(in_features=52, out_features=52, bias=True)\n",
      "  (linear_first): Linear(in_features=104, out_features=256, bias=True)\n",
      "  (linear_second): Linear(in_features=256, out_features=52, bias=True)\n",
      "  (linear_weight1): Linear(in_features=104, out_features=1, bias=True)\n",
      "  (linear_weight2): Linear(in_features=104, out_features=1, bias=True)\n",
      "  (linear_final): Linear(in_features=104, out_features=52, bias=True)\n",
      "  (output_layer): Linear(in_features=52, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d37eb82b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 24, train_loss = 7.0857, test_loss = 10.0767, train_f1 = 0.4205, test_f1 = 0.2787:  24%|██▍       | 24/100 [02:11<06:57,  5.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [70]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     batch_num \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     37\u001b[0m     train_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m batch_num\n\u001b[0;32m---> 38\u001b[0m     train_score \u001b[38;5;241m=\u001b[39m \u001b[43mf1_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#     train_score /= batch_num\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/articleclassifier/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1146\u001b[0m, in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf1_score\u001b[39m(\n\u001b[1;32m   1012\u001b[0m     y_true,\n\u001b[1;32m   1013\u001b[0m     y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1020\u001b[0m ):\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \n\u001b[1;32m   1023\u001b[0m \u001b[38;5;124;03m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;124;03m    array([0.66666667, 1.        , 0.66666667])\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfbeta_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/articleclassifier/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1287\u001b[0m, in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfbeta_score\u001b[39m(\n\u001b[1;32m   1159\u001b[0m     y_true,\n\u001b[1;32m   1160\u001b[0m     y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1167\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1168\u001b[0m ):\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m \n\u001b[1;32m   1171\u001b[0m \u001b[38;5;124;03m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;124;03m    array([0.71..., 0.        , 0.        ])\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1287\u001b[0m     _, _, f, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf-score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "File \u001b[0;32m~/.conda/envs/articleclassifier/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1573\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m beta \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1572\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta should be >=0 in the F-beta score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1573\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1576\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/articleclassifier/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1374\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1372\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[0;32m-> 1374\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[1;32m   1377\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/.conda/envs/articleclassifier/lib/python3.9/site-packages/sklearn/metrics/_classification.py:130\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    127\u001b[0m             y_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 130\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m \u001b[43mcsr_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m csr_matrix(y_pred)\n\u001b[1;32m    132\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/articleclassifier/lib/python3.9/site-packages/scipy/sparse/_compressed.py:85\u001b[0m, in \u001b[0;36m_cs_matrix.__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munrecognized \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_matrix constructor usage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_self(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(\n\u001b[0;32m---> 85\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_coo_container\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     ))\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Read matrix dimensions given, if any\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/articleclassifier/lib/python3.9/site-packages/scipy/sparse/_coo.py:190\u001b[0m, in \u001b[0;36mcoo_matrix.__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_shape(shape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape:\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minconsistent shapes: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    188\u001b[0m                          (shape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape))\n\u001b[0;32m--> 190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol \u001b[38;5;241m=\u001b[39m \u001b[43mM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m M[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol]\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_canonical_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, weight_decay=4e-5)\n",
    "criterion = torch.nn.BCELoss(reduction='sum')\n",
    "epoch = 100\n",
    "best_acc = 0.0\n",
    "pre_acc = 0.0\n",
    "\n",
    "# if not os.path.isdir('./rcv_log'):\n",
    "#     os.makedirs('./rcv_log')\n",
    "# trace_file='./rcv_log/trace_rcv.txt'\n",
    "\n",
    "for ep in (pbar := tqdm(range(1, epoch + 1), position=0)):\n",
    "    train_loss = 0\n",
    "    train_score = 0\n",
    "    test_loss = 0\n",
    "    test_score  = 0\n",
    "    \n",
    "    model.train()\n",
    "    predictions = np.zeros((len(train_set), 52))\n",
    "    real_labels = np.zeros((len(train_set), 52))\n",
    "    for i, (data, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # data = data.cuda()\n",
    "        # labels = labels.cuda()\n",
    "\n",
    "        pred = model(data)\n",
    "        loss = criterion(pred, labels.float()) / pred.size(0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += float(loss)\n",
    "#         train_score += f1_score(labels.detach().cpu().numpy(), np.round(pred.detach().cpu().numpy()), average='macro', zero_division=0)\n",
    "        predictions[i*batch_size: (i+1)*batch_size, :] = np.round(pred.detach().cpu().numpy())\n",
    "        real_labels[i*batch_size: (i+1)*batch_size, :] = labels.detach().cpu().numpy()\n",
    "        \n",
    "    batch_num = i + 1\n",
    "    train_loss /= batch_num\n",
    "    train_score = f1_score(real_labels, predictions, average='macro', zero_division=0)\n",
    "#     train_score /= batch_num\n",
    "\n",
    "    \n",
    "    test_loss = 0\n",
    "    test_score = 0\n",
    "    test_predictions = np.zeros((len(test_set), 52))\n",
    "    test_real_labels = np.zeros((len(test_set), 52))\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (data, labels) in enumerate(test_loader):\n",
    "            # data = data.cuda()\n",
    "            # labels = labels.cuda()\n",
    "            pred = model(data)\n",
    "            loss = criterion(pred, labels.float()) / pred.size(0)\n",
    "\n",
    "            # 计算metric\n",
    "            labels_cpu = labels.data.cpu().numpy()\n",
    "            pred_cpu = np.round(pred.data.cpu().numpy())\n",
    "            \n",
    "#             test_score += f1_score(labels_cpu, pred_cpu, average='macro', zero_division=0)\n",
    "            test_loss += float(loss)\n",
    "            test_predictions[i*64: (i+1)*64, :] = np.round(pred.detach().cpu().numpy())\n",
    "            test_real_labels[i*64: (i+1)*64, :] = labels.detach().cpu().numpy()\n",
    "            \n",
    "    batch_num = i + 1\n",
    "    test_loss /= batch_num\n",
    "#     test_score /= batch_num\n",
    "    test_score = f1_score(test_real_labels, test_predictions, average='macro', zero_division=0)\n",
    "\n",
    "\n",
    "    pbar.set_description(f\"epoch {ep}, train_loss = {train_loss:.4f}, test_loss = {test_loss:.4f}, train_f1 = {train_score:.4f}, test_f1 = {test_score:.4f}\")\n",
    "\n",
    "\n",
    "    if test_p3 < pre_acc:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.0001\n",
    "    pre_acc = test_p3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9fbd40e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/157 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[141632, 236289, 428078,  ...,      0,      0,      0],\n",
      "        [385728,  70288,  84518,  ...,      0,      0,      0],\n",
      "        [  4558, 345890, 205374,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [303372, 199293, 386063,  ...,      0,      0,      0],\n",
      "        [  1822, 431470, 299190,  ...,      0,      0,      0],\n",
      "        [197196, 378696, 376918,  ...,      0,      0,      0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/157 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 52, 104)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(data, embedding_generation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(pred\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for i, (data, labels) in enumerate(tqdm(test_loader)):\n",
    "    print(data)\n",
    "    pred = model(data, embedding_generation=True)\n",
    "    print(pred.detach().numpy().shape)\n",
    "    assert False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a4a5da76",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, cc_path(f'models/xml_embedding/word_embeddings_{pretrain}_20230411_all_data.txt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e78adeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(cc_path(f'models/xml_embedding/word_embeddings_{pretrain}_20230408_all_data.txt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79227d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "del embedding_df, processed_df, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "59380f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_batch_size = 1024\n",
    "abstracts_to_embed = np.array(abstracts_df.loc[:, 'abstract'].to_list(), dtype=int)\n",
    "puis_to_embed = np.array(abstracts_df.loc[:, 'pui'].to_list(), dtype=int)\n",
    "\n",
    "embedding_data = data_utils.TensorDataset(torch.from_numpy(abstracts_to_embed).type(torch.LongTensor), \n",
    "                                          torch.from_numpy(puis_to_embed).type(torch.LongTensor))\n",
    "final_data = data_utils.DataLoader(embedding_data, emb_batch_size, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2a39198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b619ece4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117310"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(abstracts_df['pui'].to_numpy(dtype=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e44a1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 91/115 [6:56:21<2:36:28, 391.21s/it]"
     ]
    }
   ],
   "source": [
    "num_of_embedding_dim = 104\n",
    "\n",
    "embedding_columns =  [f'd_{i}' for i in range(52)]\n",
    "xml_embedding_df = pd.DataFrame(columns=embedding_columns, index=abstracts_df['pui'].to_numpy(dtype=str))\n",
    "# xml_embedding_df['embedding'] = xml_embedding_df['embedding'].astype(object)\n",
    "np.set_printoptions(threshold = 100000000000000)\n",
    "\n",
    "for i, (data, pui) in enumerate(tqdm(final_data)):\n",
    "    pred = model(data, embedding_generation=True)\n",
    "    \n",
    "    right_puis =  list(pui.detach().numpy())\n",
    "    \n",
    "    numpy_preds = pred.detach().numpy()\n",
    "    for idx_batch in range(numpy_preds.shape[0]):\n",
    "        for idx_label in range(numpy_preds.shape[1]):\n",
    "            xml_embedding_df.loc[right_puis[idx_batch], embedding_columns[idx_label]] = numpy_preds[idx_batch, \n",
    "                                                                                             idx_label, \n",
    "                                                                                             :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0ae5ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_embedding_df.reset_index(names='pui', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f946e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xml_embedding_df.to_feather(cc_path('data/processed/canary/embeddings_xml_20230411.ftr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a173c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.width = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b378a851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d_0</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>d_5</th>\n",
       "      <th>d_6</th>\n",
       "      <th>d_7</th>\n",
       "      <th>d_8</th>\n",
       "      <th>d_9</th>\n",
       "      <th>...</th>\n",
       "      <th>d_42</th>\n",
       "      <th>d_43</th>\n",
       "      <th>d_44</th>\n",
       "      <th>d_45</th>\n",
       "      <th>d_46</th>\n",
       "      <th>d_47</th>\n",
       "      <th>d_48</th>\n",
       "      <th>d_49</th>\n",
       "      <th>d_50</th>\n",
       "      <th>d_51</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>624531411</th>\n",
       "      <td>[-0.016383378, -0.11197758, -0.037253555, -0.2...</td>\n",
       "      <td>[0.010617012, -0.09705223, -0.01200425, -0.120...</td>\n",
       "      <td>[0.028110279, -0.08243386, -0.016131265, -0.18...</td>\n",
       "      <td>[0.0022158436, -0.11320748, -0.010183357, -0.1...</td>\n",
       "      <td>[0.005926796, -0.10500478, -0.010475147, -0.12...</td>\n",
       "      <td>[0.0113420375, -0.09601049, -0.014044623, -0.1...</td>\n",
       "      <td>[0.016601732, -0.09619771, -0.009100688, -0.14...</td>\n",
       "      <td>[0.11544543, -0.005443737, -0.005527966, -0.18...</td>\n",
       "      <td>[0.02586015, -0.086351044, -0.010884014, -0.16...</td>\n",
       "      <td>[0.009882777, -0.09707534, -0.021866538, -0.18...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.18155344, 0.026362767, -0.01475883, -0.2147...</td>\n",
       "      <td>[0.0069437977, -0.09799223, -0.02816795, -0.23...</td>\n",
       "      <td>[0.18892401, 0.041114308, -0.00097465096, -0.1...</td>\n",
       "      <td>[0.11469094, -0.015334345, 0.006475803, -0.087...</td>\n",
       "      <td>[0.027374564, -0.08158906, -0.01762917, -0.206...</td>\n",
       "      <td>[0.04705087, -0.065758094, -0.018019354, -0.21...</td>\n",
       "      <td>[0.3474198, 0.15047382, 0.0039090053, -0.13961...</td>\n",
       "      <td>[0.11787519, -0.0064049475, -0.0025091413, -0....</td>\n",
       "      <td>[0.069682136, -0.054890573, -0.011223326, -0.1...</td>\n",
       "      <td>[-0.011351064, -0.100169934, -0.034848228, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625340088</th>\n",
       "      <td>[0.06821822, -0.05620522, -0.065800205, -0.222...</td>\n",
       "      <td>[0.10693053, -0.017095909, -0.044121563, -0.09...</td>\n",
       "      <td>[0.10340122, -0.019759048, -0.055330824, -0.16...</td>\n",
       "      <td>[0.044841465, -0.09599258, -0.047074445, -0.14...</td>\n",
       "      <td>[0.042860292, -0.08393292, -0.043275695, -0.10...</td>\n",
       "      <td>[0.038487554, -0.07838504, -0.053617507, -0.16...</td>\n",
       "      <td>[0.05473292, -0.075368986, -0.042547263, -0.12...</td>\n",
       "      <td>[0.18368945, 0.09457679, -0.079515636, -0.1797...</td>\n",
       "      <td>[0.1327912, 0.019922812, -0.06676929, -0.15574...</td>\n",
       "      <td>[0.1424106, 0.018941574, -0.052223623, -0.1458...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.24319527, 0.12634419, -0.05570019, -0.18991...</td>\n",
       "      <td>[0.12861286, 0.009207115, -0.063367575, -0.198...</td>\n",
       "      <td>[0.24438602, 0.14896263, -0.07208059, -0.14055...</td>\n",
       "      <td>[0.1749304, 0.08700106, -0.05070314, -0.066946...</td>\n",
       "      <td>[0.100104034, -0.02964319, -0.0711903, -0.2004...</td>\n",
       "      <td>[0.13491955, 0.01794957, -0.067108504, -0.1916...</td>\n",
       "      <td>[0.41909015, 0.33678198, -0.07781066, -0.12629...</td>\n",
       "      <td>[0.17230125, 0.06469397, -0.08423577, -0.16590...</td>\n",
       "      <td>[0.15537813, 0.036530755, -0.06381854, -0.1704...</td>\n",
       "      <td>[0.02809963, -0.08056283, -0.07836892, -0.2682...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625805682</th>\n",
       "      <td>[0.028528668, -0.060206894, 0.07548873, -0.348...</td>\n",
       "      <td>[0.042353593, -0.07522996, 0.045501076, -0.324...</td>\n",
       "      <td>[0.043712128, -0.07069669, 0.053283475, -0.341...</td>\n",
       "      <td>[0.0912251, -0.05125811, 0.106232695, -0.31711...</td>\n",
       "      <td>[0.071796775, -0.050604787, 0.09280021, -0.337...</td>\n",
       "      <td>[0.076959535, -0.04865962, 0.090105645, -0.341...</td>\n",
       "      <td>[0.077228464, -0.051366046, 0.099378556, -0.32...</td>\n",
       "      <td>[0.051017053, -0.06567388, 0.10099919, -0.2431...</td>\n",
       "      <td>[0.04003511, -0.047263972, 0.09858238, -0.3281...</td>\n",
       "      <td>[0.054769102, -0.05078943, 0.08115368, -0.2868...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.18210159, 0.063631855, 0.05955499, -0.32273...</td>\n",
       "      <td>[0.057935737, -0.034007467, 0.08288933, -0.324...</td>\n",
       "      <td>[0.21283509, 0.080757335, 0.084314525, -0.2392...</td>\n",
       "      <td>[0.099942975, -0.049863294, 0.13453224, -0.256...</td>\n",
       "      <td>[0.046176944, -0.0769093, 0.06805101, -0.29428...</td>\n",
       "      <td>[0.037154716, -0.05500753, 0.088356555, -0.317...</td>\n",
       "      <td>[0.33445185, 0.20716941, 0.06621084, -0.234082...</td>\n",
       "      <td>[0.07703457, -0.032216277, 0.1353079, -0.22812...</td>\n",
       "      <td>[0.06902366, -0.051700268, 0.06834428, -0.3177...</td>\n",
       "      <td>[0.03009884, -0.06154553, 0.08346843, -0.34348...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626662493</th>\n",
       "      <td>[0.059955165, -0.02994335, -0.03311316, -0.143...</td>\n",
       "      <td>[0.10139962, 0.009669423, -0.022478202, -0.038...</td>\n",
       "      <td>[0.0790334, -0.01266022, -0.02287779, -0.10124...</td>\n",
       "      <td>[0.030228794, -0.07793985, -0.028091416, -0.13...</td>\n",
       "      <td>[0.036995795, -0.06270175, -0.0315825, -0.1039...</td>\n",
       "      <td>[0.027064249, -0.0709749, -0.031441428, -0.146...</td>\n",
       "      <td>[0.043158136, -0.059222713, -0.027855176, -0.1...</td>\n",
       "      <td>[0.13726412, 0.05951462, 0.010710327, -0.14864...</td>\n",
       "      <td>[0.083741456, 0.0010779575, 0.00037083589, -0....</td>\n",
       "      <td>[0.12572579, 0.037525304, -0.019792218, -0.073...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.15025625, 0.0521947, -0.026349314, -0.17327...</td>\n",
       "      <td>[0.11670811, 0.030976295, -0.02641234, -0.1190...</td>\n",
       "      <td>[0.19017154, 0.094812796, -0.003392328, -0.117...</td>\n",
       "      <td>[0.13538006, 0.04384046, 0.006757291, -0.05630...</td>\n",
       "      <td>[0.07334437, -0.024985045, -0.021543493, -0.13...</td>\n",
       "      <td>[0.097782776, 0.006623201, -0.026629627, -0.12...</td>\n",
       "      <td>[0.3373683, 0.24063022, -0.0021311226, -0.1236...</td>\n",
       "      <td>[0.15725388, 0.042757872, -0.008270113, -0.101...</td>\n",
       "      <td>[0.12123768, 0.029323883, -0.025152106, -0.115...</td>\n",
       "      <td>[0.022587577, -0.067165665, -0.03826805, -0.19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626822402</th>\n",
       "      <td>[0.27494726, -0.061647326, 0.18791562, -0.2993...</td>\n",
       "      <td>[0.3305084, -0.046140015, 0.18127444, -0.38189...</td>\n",
       "      <td>[0.30549046, -0.052407146, 0.18561853, -0.3563...</td>\n",
       "      <td>[0.33089703, -0.05900829, 0.19056454, -0.28315...</td>\n",
       "      <td>[0.3212989, -0.053632278, 0.18682936, -0.28965...</td>\n",
       "      <td>[0.33208808, -0.05664587, 0.18075578, -0.28696...</td>\n",
       "      <td>[0.32238388, -0.05355747, 0.18911934, -0.28512...</td>\n",
       "      <td>[0.2843451, -0.075362384, 0.17306942, -0.26655...</td>\n",
       "      <td>[0.26771063, -0.066218354, 0.17441167, -0.2618...</td>\n",
       "      <td>[0.28459507, -0.07167025, 0.17930306, -0.29999...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.34554872, -0.114436746, 0.16829908, -0.2782...</td>\n",
       "      <td>[0.25267428, -0.080995634, 0.17711401, -0.2627...</td>\n",
       "      <td>[0.3340537, -0.093892336, 0.1561747, -0.314642...</td>\n",
       "      <td>[0.3147732, -0.065012425, 0.17388539, -0.25686...</td>\n",
       "      <td>[0.29047766, -0.05335179, 0.18333776, -0.34369...</td>\n",
       "      <td>[0.2530854, -0.07009541, 0.18177888, -0.267674...</td>\n",
       "      <td>[0.3673926, -0.15189306, 0.14653346, -0.250014...</td>\n",
       "      <td>[0.27901053, -0.060020022, 0.20105505, -0.2875...</td>\n",
       "      <td>[0.2838174, -0.074696735, 0.17902729, -0.29193...</td>\n",
       "      <td>[0.25677127, -0.058577746, 0.19382006, -0.2792...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011621972</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011622024</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011622065</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011626864</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011632199</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117310 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          d_0   \n",
       "624531411   [-0.016383378, -0.11197758, -0.037253555, -0.2...  \\\n",
       "625340088   [0.06821822, -0.05620522, -0.065800205, -0.222...   \n",
       "625805682   [0.028528668, -0.060206894, 0.07548873, -0.348...   \n",
       "626662493   [0.059955165, -0.02994335, -0.03311316, -0.143...   \n",
       "626822402   [0.27494726, -0.061647326, 0.18791562, -0.2993...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                          d_1   \n",
       "624531411   [0.010617012, -0.09705223, -0.01200425, -0.120...  \\\n",
       "625340088   [0.10693053, -0.017095909, -0.044121563, -0.09...   \n",
       "625805682   [0.042353593, -0.07522996, 0.045501076, -0.324...   \n",
       "626662493   [0.10139962, 0.009669423, -0.022478202, -0.038...   \n",
       "626822402   [0.3305084, -0.046140015, 0.18127444, -0.38189...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                          d_2   \n",
       "624531411   [0.028110279, -0.08243386, -0.016131265, -0.18...  \\\n",
       "625340088   [0.10340122, -0.019759048, -0.055330824, -0.16...   \n",
       "625805682   [0.043712128, -0.07069669, 0.053283475, -0.341...   \n",
       "626662493   [0.0790334, -0.01266022, -0.02287779, -0.10124...   \n",
       "626822402   [0.30549046, -0.052407146, 0.18561853, -0.3563...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                          d_3   \n",
       "624531411   [0.0022158436, -0.11320748, -0.010183357, -0.1...  \\\n",
       "625340088   [0.044841465, -0.09599258, -0.047074445, -0.14...   \n",
       "625805682   [0.0912251, -0.05125811, 0.106232695, -0.31711...   \n",
       "626662493   [0.030228794, -0.07793985, -0.028091416, -0.13...   \n",
       "626822402   [0.33089703, -0.05900829, 0.19056454, -0.28315...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                          d_4   \n",
       "624531411   [0.005926796, -0.10500478, -0.010475147, -0.12...  \\\n",
       "625340088   [0.042860292, -0.08393292, -0.043275695, -0.10...   \n",
       "625805682   [0.071796775, -0.050604787, 0.09280021, -0.337...   \n",
       "626662493   [0.036995795, -0.06270175, -0.0315825, -0.1039...   \n",
       "626822402   [0.3212989, -0.053632278, 0.18682936, -0.28965...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                          d_5   \n",
       "624531411   [0.0113420375, -0.09601049, -0.014044623, -0.1...  \\\n",
       "625340088   [0.038487554, -0.07838504, -0.053617507, -0.16...   \n",
       "625805682   [0.076959535, -0.04865962, 0.090105645, -0.341...   \n",
       "626662493   [0.027064249, -0.0709749, -0.031441428, -0.146...   \n",
       "626822402   [0.33208808, -0.05664587, 0.18075578, -0.28696...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                          d_6   \n",
       "624531411   [0.016601732, -0.09619771, -0.009100688, -0.14...  \\\n",
       "625340088   [0.05473292, -0.075368986, -0.042547263, -0.12...   \n",
       "625805682   [0.077228464, -0.051366046, 0.099378556, -0.32...   \n",
       "626662493   [0.043158136, -0.059222713, -0.027855176, -0.1...   \n",
       "626822402   [0.32238388, -0.05355747, 0.18911934, -0.28512...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                          d_7   \n",
       "624531411   [0.11544543, -0.005443737, -0.005527966, -0.18...  \\\n",
       "625340088   [0.18368945, 0.09457679, -0.079515636, -0.1797...   \n",
       "625805682   [0.051017053, -0.06567388, 0.10099919, -0.2431...   \n",
       "626662493   [0.13726412, 0.05951462, 0.010710327, -0.14864...   \n",
       "626822402   [0.2843451, -0.075362384, 0.17306942, -0.26655...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                          d_8   \n",
       "624531411   [0.02586015, -0.086351044, -0.010884014, -0.16...  \\\n",
       "625340088   [0.1327912, 0.019922812, -0.06676929, -0.15574...   \n",
       "625805682   [0.04003511, -0.047263972, 0.09858238, -0.3281...   \n",
       "626662493   [0.083741456, 0.0010779575, 0.00037083589, -0....   \n",
       "626822402   [0.26771063, -0.066218354, 0.17441167, -0.2618...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                          d_9  ...   \n",
       "624531411   [0.009882777, -0.09707534, -0.021866538, -0.18...  ...  \\\n",
       "625340088   [0.1424106, 0.018941574, -0.052223623, -0.1458...  ...   \n",
       "625805682   [0.054769102, -0.05078943, 0.08115368, -0.2868...  ...   \n",
       "626662493   [0.12572579, 0.037525304, -0.019792218, -0.073...  ...   \n",
       "626822402   [0.28459507, -0.07167025, 0.17930306, -0.29999...  ...   \n",
       "...                                                       ...  ...   \n",
       "2011621972                                                NaN  ...   \n",
       "2011622024                                                NaN  ...   \n",
       "2011622065                                                NaN  ...   \n",
       "2011626864                                                NaN  ...   \n",
       "2011632199                                                NaN  ...   \n",
       "\n",
       "                                                         d_42   \n",
       "624531411   [0.18155344, 0.026362767, -0.01475883, -0.2147...  \\\n",
       "625340088   [0.24319527, 0.12634419, -0.05570019, -0.18991...   \n",
       "625805682   [0.18210159, 0.063631855, 0.05955499, -0.32273...   \n",
       "626662493   [0.15025625, 0.0521947, -0.026349314, -0.17327...   \n",
       "626822402   [0.34554872, -0.114436746, 0.16829908, -0.2782...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                         d_43   \n",
       "624531411   [0.0069437977, -0.09799223, -0.02816795, -0.23...  \\\n",
       "625340088   [0.12861286, 0.009207115, -0.063367575, -0.198...   \n",
       "625805682   [0.057935737, -0.034007467, 0.08288933, -0.324...   \n",
       "626662493   [0.11670811, 0.030976295, -0.02641234, -0.1190...   \n",
       "626822402   [0.25267428, -0.080995634, 0.17711401, -0.2627...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                         d_44   \n",
       "624531411   [0.18892401, 0.041114308, -0.00097465096, -0.1...  \\\n",
       "625340088   [0.24438602, 0.14896263, -0.07208059, -0.14055...   \n",
       "625805682   [0.21283509, 0.080757335, 0.084314525, -0.2392...   \n",
       "626662493   [0.19017154, 0.094812796, -0.003392328, -0.117...   \n",
       "626822402   [0.3340537, -0.093892336, 0.1561747, -0.314642...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                         d_45   \n",
       "624531411   [0.11469094, -0.015334345, 0.006475803, -0.087...  \\\n",
       "625340088   [0.1749304, 0.08700106, -0.05070314, -0.066946...   \n",
       "625805682   [0.099942975, -0.049863294, 0.13453224, -0.256...   \n",
       "626662493   [0.13538006, 0.04384046, 0.006757291, -0.05630...   \n",
       "626822402   [0.3147732, -0.065012425, 0.17388539, -0.25686...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                         d_46   \n",
       "624531411   [0.027374564, -0.08158906, -0.01762917, -0.206...  \\\n",
       "625340088   [0.100104034, -0.02964319, -0.0711903, -0.2004...   \n",
       "625805682   [0.046176944, -0.0769093, 0.06805101, -0.29428...   \n",
       "626662493   [0.07334437, -0.024985045, -0.021543493, -0.13...   \n",
       "626822402   [0.29047766, -0.05335179, 0.18333776, -0.34369...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                         d_47   \n",
       "624531411   [0.04705087, -0.065758094, -0.018019354, -0.21...  \\\n",
       "625340088   [0.13491955, 0.01794957, -0.067108504, -0.1916...   \n",
       "625805682   [0.037154716, -0.05500753, 0.088356555, -0.317...   \n",
       "626662493   [0.097782776, 0.006623201, -0.026629627, -0.12...   \n",
       "626822402   [0.2530854, -0.07009541, 0.18177888, -0.267674...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                         d_48   \n",
       "624531411   [0.3474198, 0.15047382, 0.0039090053, -0.13961...  \\\n",
       "625340088   [0.41909015, 0.33678198, -0.07781066, -0.12629...   \n",
       "625805682   [0.33445185, 0.20716941, 0.06621084, -0.234082...   \n",
       "626662493   [0.3373683, 0.24063022, -0.0021311226, -0.1236...   \n",
       "626822402   [0.3673926, -0.15189306, 0.14653346, -0.250014...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                         d_49   \n",
       "624531411   [0.11787519, -0.0064049475, -0.0025091413, -0....  \\\n",
       "625340088   [0.17230125, 0.06469397, -0.08423577, -0.16590...   \n",
       "625805682   [0.07703457, -0.032216277, 0.1353079, -0.22812...   \n",
       "626662493   [0.15725388, 0.042757872, -0.008270113, -0.101...   \n",
       "626822402   [0.27901053, -0.060020022, 0.20105505, -0.2875...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                         d_50   \n",
       "624531411   [0.069682136, -0.054890573, -0.011223326, -0.1...  \\\n",
       "625340088   [0.15537813, 0.036530755, -0.06381854, -0.1704...   \n",
       "625805682   [0.06902366, -0.051700268, 0.06834428, -0.3177...   \n",
       "626662493   [0.12123768, 0.029323883, -0.025152106, -0.115...   \n",
       "626822402   [0.2838174, -0.074696735, 0.17902729, -0.29193...   \n",
       "...                                                       ...   \n",
       "2011621972                                                NaN   \n",
       "2011622024                                                NaN   \n",
       "2011622065                                                NaN   \n",
       "2011626864                                                NaN   \n",
       "2011632199                                                NaN   \n",
       "\n",
       "                                                         d_51  \n",
       "624531411   [-0.011351064, -0.100169934, -0.034848228, -0....  \n",
       "625340088   [0.02809963, -0.08056283, -0.07836892, -0.2682...  \n",
       "625805682   [0.03009884, -0.06154553, 0.08346843, -0.34348...  \n",
       "626662493   [0.022587577, -0.067165665, -0.03826805, -0.19...  \n",
       "626822402   [0.25677127, -0.058577746, 0.19382006, -0.2792...  \n",
       "...                                                       ...  \n",
       "2011621972                                                NaN  \n",
       "2011622024                                                NaN  \n",
       "2011622065                                                NaN  \n",
       "2011626864                                                NaN  \n",
       "2011632199                                                NaN  \n",
       "\n",
       "[117310 rows x 52 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_embedding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf32b60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-articleclassifier [Python]",
   "language": "python",
   "name": "conda-env-.conda-articleclassifier-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
