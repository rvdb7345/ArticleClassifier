{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6a82764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers.data import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import EarlyStoppingCallback, AutoTokenizer, DataCollatorWithPadding, AdamW\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/home/jovyan/20230406_ArticleClassifier/ArticleClassifier\")\n",
    "from src.general.utils import cc_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5eb2ca82",
   "metadata": {},
   "outputs": [],
   "source": [
    " labels = ['human', 'mouse', 'rat', 'nonhuman', 'controlled study',\n",
    "           'animal experiment', 'animal tissue', 'animal model', 'animal cell',\n",
    "           'major clinical study', 'clinical article', 'case report',\n",
    "           'multicenter study', 'systematic review', 'meta analysis',\n",
    "           'observational study', 'pilot study', 'longitudinal study',\n",
    "           'retrospective study', 'case control study', 'cohort analysis',\n",
    "           'cross-sectional study', 'diagnostic test accuracy study',\n",
    "           'double blind procedure', 'crossover procedure',\n",
    "           'single blind procedure', 'adult', 'aged', 'middle aged', 'child',\n",
    "           'adolescent', 'young adult', 'very elderly', 'infant', 'school child',\n",
    "           'newborn', 'preschool child', 'embryo', 'fetus', 'male', 'female',\n",
    "           'human cell', 'human tissue', 'normal human', 'human experiment',\n",
    "           'phase 2 clinical trial', 'randomized controlled trial',\n",
    "           'clinical trial', 'controlled clinical trial', 'phase 3 clinical trial',\n",
    "           'phase 1 clinical trial', 'phase 4 clinical trial']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1857910",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "209e5ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "27bc9405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_whole_ds(DATA_PATH = \"../data/\"):\n",
    "\n",
    "        total = pd.read_csv(cc_path(f\"data/processed/canary/articles_cleaned.csv\"))\n",
    "        total.pui = total.pui.astype(str)\n",
    "        \n",
    "        total[labels] = total[labels].astype(int)\n",
    "        \n",
    "        total['str_keywords'] = total['keywords'].str.replace('[', ' ').str.replace(']', ' ').str.replace(', ', ' ').str.replace(\"'\", '')\n",
    "        total['embedding_text'] = total['title'] + total['str_keywords'] + total['abstract']\n",
    "\n",
    "\n",
    "        with open(cc_path(f'data/train_indices.txt')) as f:\n",
    "            train_puis = f.read().splitlines()\n",
    "            # print(train_puis)\n",
    "        with open(cc_path(f'data/val_indices.txt')) as f:\n",
    "            val_puis = f.read().splitlines()\n",
    "        with open(cc_path(f'data/test_indices.txt')) as f:\n",
    "            test_puis = f.read().splitlines()\n",
    "\n",
    "        # Split data into train-validation-test sets\n",
    "        train = total.loc[total.pui.isin(train_puis), :]\n",
    "        val = total.loc[total.pui.isin(val_puis), :]\n",
    "        test = total.loc[total.pui.isin(test_puis), :]\n",
    "        \n",
    "        return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4153bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_small_ds(DATA_PATH = \"../data/\"):\n",
    "\n",
    "    small_df = pd.read_csv(DATA_PATH + 'small_dataset.csv')\n",
    "    all_x = small_df.iloc[:,-1]\n",
    "    all_y = small_df.iloc[:,:-1]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(all_x, all_y,\n",
    "    test_size=0.2, shuffle = True, random_state = 8)\n",
    "\n",
    "    # Use the same function above for the validation set\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "    test_size=0.25, random_state= 8) # 0.25 x 0.8 = 0.2\n",
    "    \n",
    "    train_df = y_train\n",
    "    train_df['input_raw'] = X_train\n",
    "    test_df = y_test\n",
    "    test_df['input_raw'] = X_test\n",
    "    val_df = y_val\n",
    "    val_df['input_raw'] = X_val\n",
    "    \n",
    "    return train_df, test_df, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cd21a9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.2 s, sys: 510 ms, total: 7.71 s\n",
      "Wall time: 7.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Choose whether you want to load the whole dataset for the experiment\n",
    "train_df, test_df, valid_df = load_whole_ds()\n",
    "# train_df, test_df, valid_df = load_small_ds(DATA_PATH)\n",
    "\n",
    "trds = Dataset.from_pandas(train_df)\n",
    "vds = Dataset.from_pandas(valid_df)\n",
    "teds = Dataset.from_pandas(test_df)\n",
    "\n",
    "full_ds = DatasetDict()\n",
    "\n",
    "full_ds['train'] = trds\n",
    "full_ds['validation'] = vds\n",
    "full_ds['test'] = teds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "251a70c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc84eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d44ba501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 137 ms, sys: 8.22 ms, total: 145 ms\n",
      "Wall time: 147 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#pretrained_model_name = \"microsoft/xtremedistil-l6-h256-uncased\"  # for xtremedistil transformer\n",
    "model_version = '../data_preparation/scibert_scivocab_uncased'\n",
    "do_lower_case = True\n",
    "# model = BertModel.from_pretrained(model_version)\n",
    "# trained_bert_model = torch.load(cc_path(f'models/embedders/finetuned_bert_56k_20e_3lay_best_iter.pt'), map_location=torch.device('cpu'))\n",
    "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    # take a batch of texts\n",
    "    text = examples[\"embedding_text\"]\n",
    "    # encode them\n",
    "    encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    # add labels\n",
    "    labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "    # create numpy array of shape (batch_size, num_labels)\n",
    "    labels_matrix = np.zeros((len(text), len(labels)))\n",
    "    # fill numpy array\n",
    "    for idx, label in enumerate(labels):\n",
    "        labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "    encoding[\"labels\"] = labels_matrix.tolist()\n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "84df1f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485f3ec2cc9541f98c15942e916838c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36055 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69770ccc5edb40ac8b8104226783247c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11268 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "451fdad160684914949dcb17690b4bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 39s, sys: 608 ms, total: 5min 39s\n",
      "Wall time: 5min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "    encoded_dataset = full_ds.map(preprocess_data, batched=True, remove_columns=full_ds['train'].column_names)\n",
    "except:\n",
    "    encoded_dataset = full_ds.map(preprocess_data, batched=True, remove_columns=full_ds['test'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1df5e725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'validation', 'test'])\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "print(encoded_dataset.keys())\n",
    "print(encoded_dataset['test'][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "16998be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "pretrained_model_name = \"allenai/scibert_scivocab_cased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name, \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "752afc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'layer.9' in name or 'layer.10' in name or \n",
    "for name, param in model.named_parameters():\n",
    "    if 'layer.10' in name or 'layer.9' in name or 'layer.11' in name or 'classifier.weight' in name or 'classifier.bias' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0273d83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dea18e",
   "metadata": {},
   "source": [
    "# Loss functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b02118a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use custom loss\n",
    "\n",
    "class F1Trainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "    \n",
    "        S=-1\n",
    "        E=0\n",
    "        y = labels\n",
    "        # Sigmoid hyperparams:\n",
    "        b = torch.tensor(S)\n",
    "        c = torch.tensor(E)\n",
    "\n",
    "        # Calculate the sigmoid\n",
    "        sig = 1 / (1 + torch.exp(b * (logits + c)))\n",
    "        tp = torch.sum(sig * y, dim=0)\n",
    "        fp = torch.sum(sig * (1 - y), dim=0)\n",
    "        fn = torch.sum((1 - sig) * y, dim=0)\n",
    "\n",
    "        sigmoid_f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "        cost = 1 - sigmoid_f1\n",
    "        macroCost = torch.mean(cost)\n",
    "\n",
    "        return (macroCost, outputs) if return_outputs else macroCost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ac1d0f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 2\n",
    "alpha = 0.75\n",
    "\n",
    "class FLTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        y = labels\n",
    "        y_hat = outputs.get(\"logits\")\n",
    "        \n",
    "        BCEWithLogitsLoss = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "        bce = BCEWithLogitsLoss(y_hat, y)\n",
    "        \n",
    "        pt = torch.exp(-bce)\n",
    "        \n",
    "        alpha_factor = y * alpha + (1 - y) * (1 - alpha)\n",
    "        modulating_factor = torch.pow((1.0 - pt), gamma)\n",
    "\n",
    "        focal_loss = torch.mean(alpha_factor * modulating_factor * bce)\n",
    "#         focal_loss = torch.mean(modulating_factor * ce)\n",
    "\n",
    "        return (focal_loss, outputs) if return_outputs else focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ec08494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_neg=4\n",
    "gamma_pos=1\n",
    "clip=0.05\n",
    "eps=1e-8\n",
    "disable_torch_grad_focal_loss = True\n",
    "\n",
    "class ASLTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        y = labels\n",
    "        y_hat = outputs.get(\"logits\")\n",
    "        \n",
    "        # Calculating Probabilities\n",
    "        x_sigmoid = torch.sigmoid(y_hat)\n",
    "        xs_pos = x_sigmoid\n",
    "        xs_neg = 1 - x_sigmoid\n",
    "\n",
    "        # Asymmetric Clipping\n",
    "        if clip is not None and clip > 0:\n",
    "            xs_neg = (xs_neg + clip).clamp(max=1)\n",
    "\n",
    "        # Basic CE calculation\n",
    "        los_pos = y * torch.log(xs_pos.clamp(min=eps))\n",
    "        los_neg = (1 - y) * torch.log(xs_neg.clamp(min=eps))\n",
    "        loss = los_pos + los_neg\n",
    "\n",
    "        # Asymmetric Focusing\n",
    "        if gamma_neg > 0 or gamma_pos > 0:\n",
    "            if disable_torch_grad_focal_loss:\n",
    "                torch.set_grad_enabled(False)\n",
    "            pt0 = xs_pos * y\n",
    "            pt1 = xs_neg * (1 - y)  # pt = p if t > 0 else 1-p\n",
    "            pt = pt0 + pt1\n",
    "            one_sided_gamma = gamma_pos * y + gamma_neg * (1 - y)\n",
    "            one_sided_w = torch.pow(1 - pt, one_sided_gamma)\n",
    "            if disable_torch_grad_focal_loss:\n",
    "                torch.set_grad_enabled(True)\n",
    "            loss *= one_sided_w\n",
    "        \n",
    "        asl_loss = -loss.mean()\n",
    "        \n",
    "        return (asl_loss, outputs) if return_outputs else asl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9ddc96d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use custom loss for HAMMING LOSS\n",
    "\n",
    "class HLTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "    \n",
    "        S=-1\n",
    "        E=0\n",
    "        y = labels\n",
    "        # Sigmoid hyperparams:\n",
    "        b = torch.tensor(S)\n",
    "        c = torch.tensor(E)\n",
    "\n",
    "        # Calculate the sigmoid\n",
    "        sig = 1 / (1 + torch.exp(b * (logits + c)))\n",
    "\n",
    "        fp = torch.sum(sig * (1 - y), dim=0)\n",
    "        fn = torch.sum((1 - sig) * y, dim=0)\n",
    "        \n",
    "        hamm_loss = (fp + fn) / torch.sum(labels, dim=0).clamp(min=0.5)  #avoid dividing by 0 if there is no label for the class\n",
    "        \n",
    "        macroCost = torch.mean(hamm_loss)\n",
    "\n",
    "        return (macroCost, outputs) if return_outputs else macroCost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "831d4748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use custom loss\n",
    "def get_classWeights():\n",
    "    train_labels = train_df[labels]\n",
    "    tot = sum(train_labels.sum(axis=0))\n",
    "    weight = 1 - (train_labels.sum(axis=0) / tot)\n",
    "    \n",
    "    return torch.tensor(weight)\n",
    "\n",
    "weights = get_classWeights().to(device)\n",
    "weights = weights**2\n",
    "\n",
    "class F1weightTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "    \n",
    "        S=-1\n",
    "        E=0\n",
    "        y = labels\n",
    "        # Sigmoid hyperparams:\n",
    "        b = torch.tensor(S)\n",
    "        c = torch.tensor(E)\n",
    "\n",
    "        # Calculate the sigmoid\n",
    "        sig = 1 / (1 + torch.exp(b * (logits + c)))\n",
    "        tp = torch.sum(sig * y, dim=0)\n",
    "        fp = torch.sum(sig * (1 - y), dim=0)\n",
    "        fn = torch.sum((1 - sig) * y, dim=0)\n",
    "\n",
    "        sigmoid_f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "        cost = 1 - sigmoid_f1\n",
    "        weighted_cost = torch.mul(cost, weights)\n",
    "        macroCost = torch.mean(weighted_cost)\n",
    "\n",
    "        return (macroCost, outputs) if return_outputs else macroCost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "016896aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1learnTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        S=-2\n",
    "        E=0\n",
    "        y = labels\n",
    "        # Sigmoid hyperparams:\n",
    "        b = torch.tensor(S)\n",
    "        c = self.model.F1thr # Learnable parameter!!!!!!!!!!!!!!!! :)\n",
    "#         c = torch.tensor(E)\n",
    "\n",
    "        # Calculate the sigmoid\n",
    "        sig = 1 / (1 + torch.exp(b * (logits + c)))\n",
    "        tp = torch.sum(sig * y, dim=0)\n",
    "        fp = torch.sum(sig * (1 - y), dim=0)\n",
    "        fn = torch.sum((1 - sig) * y, dim=0)\n",
    "\n",
    "        sigmoid_f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "        cost = 1 - sigmoid_f1\n",
    "        macroCost = torch.mean(cost)\n",
    "\n",
    "        return (macroCost, outputs) if return_outputs else macroCost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fc2cf7",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "104c2cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    f\"scibert-cased\",\n",
    "    evaluation_strategy = 'steps',\n",
    "#     eval_steps = 100,\n",
    "    eval_steps = 500,\n",
    "    save_strategy = \"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1_micro',\n",
    "    report_to=\"none\",\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True\n",
    "    #gradient_accumulation_steps=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "660c3ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "    \n",
    "# source: https://jesuslea l.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    f1_macro_average = f1_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1_micro': f1_micro_average,\n",
    "               'f1_macro': f1_macro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c3a77e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the loss function\n",
    "loss_fn = 'FL'\n",
    "# ---!!!---!!! SELECT !!!---!!!---\n",
    "\n",
    "trainers_dict = {'BCE': Trainer, 'F1': F1Trainer, 'FL': FLTrainer, 'ASL': ASLTrainer, 'HL': HLTrainer, 'F1weight': F1weightTrainer, 'F1learn': F1learnTrainer} \n",
    "\n",
    "if loss_fn == 'F1learn':\n",
    "#     model.register_parameter(name='weights', param=torch.nn.Parameter(torch.ones(52)))\n",
    "    model.register_parameter(name='F1thr', param=torch.nn.Parameter(torch.zeros(52)))\n",
    "    print(model.F1thr)\n",
    "    \n",
    "trainer_class = trainers_dict[loss_fn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2e6f0e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainer_class(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics, \n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3000000)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "aaf1a95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/envs/articleclassifier/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11270' max='11270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11270/11270 1:56:18, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.025500</td>\n",
       "      <td>0.020959</td>\n",
       "      <td>0.682471</td>\n",
       "      <td>0.288330</td>\n",
       "      <td>0.857119</td>\n",
       "      <td>0.015087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>0.018464</td>\n",
       "      <td>0.717988</td>\n",
       "      <td>0.371651</td>\n",
       "      <td>0.875582</td>\n",
       "      <td>0.040025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.017206</td>\n",
       "      <td>0.732769</td>\n",
       "      <td>0.423165</td>\n",
       "      <td>0.884937</td>\n",
       "      <td>0.059638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>0.016851</td>\n",
       "      <td>0.730325</td>\n",
       "      <td>0.465716</td>\n",
       "      <td>0.893157</td>\n",
       "      <td>0.051384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.015800</td>\n",
       "      <td>0.016089</td>\n",
       "      <td>0.745727</td>\n",
       "      <td>0.511260</td>\n",
       "      <td>0.894661</td>\n",
       "      <td>0.070021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.748439</td>\n",
       "      <td>0.534652</td>\n",
       "      <td>0.900837</td>\n",
       "      <td>0.071530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.015638</td>\n",
       "      <td>0.759480</td>\n",
       "      <td>0.547879</td>\n",
       "      <td>0.894822</td>\n",
       "      <td>0.097178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.752322</td>\n",
       "      <td>0.546899</td>\n",
       "      <td>0.899911</td>\n",
       "      <td>0.077387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.013500</td>\n",
       "      <td>0.015326</td>\n",
       "      <td>0.761460</td>\n",
       "      <td>0.555381</td>\n",
       "      <td>0.899441</td>\n",
       "      <td>0.095048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.015685</td>\n",
       "      <td>0.757961</td>\n",
       "      <td>0.559926</td>\n",
       "      <td>0.900610</td>\n",
       "      <td>0.091498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>0.015639</td>\n",
       "      <td>0.757466</td>\n",
       "      <td>0.567386</td>\n",
       "      <td>0.900785</td>\n",
       "      <td>0.092918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.763437</td>\n",
       "      <td>0.567765</td>\n",
       "      <td>0.895252</td>\n",
       "      <td>0.112354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.016003</td>\n",
       "      <td>0.764771</td>\n",
       "      <td>0.555201</td>\n",
       "      <td>0.896755</td>\n",
       "      <td>0.107472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>0.016229</td>\n",
       "      <td>0.764913</td>\n",
       "      <td>0.575816</td>\n",
       "      <td>0.899197</td>\n",
       "      <td>0.106940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.016369</td>\n",
       "      <td>0.761839</td>\n",
       "      <td>0.575663</td>\n",
       "      <td>0.898877</td>\n",
       "      <td>0.105698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.016658</td>\n",
       "      <td>0.763564</td>\n",
       "      <td>0.569925</td>\n",
       "      <td>0.896442</td>\n",
       "      <td>0.109425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>0.016935</td>\n",
       "      <td>0.765135</td>\n",
       "      <td>0.579284</td>\n",
       "      <td>0.894259</td>\n",
       "      <td>0.112709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>0.016888</td>\n",
       "      <td>0.762087</td>\n",
       "      <td>0.576379</td>\n",
       "      <td>0.898717</td>\n",
       "      <td>0.106408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>0.017046</td>\n",
       "      <td>0.766087</td>\n",
       "      <td>0.577004</td>\n",
       "      <td>0.894669</td>\n",
       "      <td>0.114838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.765367</td>\n",
       "      <td>0.578231</td>\n",
       "      <td>0.893250</td>\n",
       "      <td>0.113862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>0.017470</td>\n",
       "      <td>0.766720</td>\n",
       "      <td>0.581616</td>\n",
       "      <td>0.893057</td>\n",
       "      <td>0.117323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>0.017333</td>\n",
       "      <td>0.766909</td>\n",
       "      <td>0.579112</td>\n",
       "      <td>0.894831</td>\n",
       "      <td>0.114306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 25min 3s, sys: 28min 1s, total: 1h 53min 4s\n",
      "Wall time: 1h 56min 18s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11270, training_loss=0.012977974861199176, metrics={'train_runtime': 6978.898, 'train_samples_per_second': 51.663, 'train_steps_per_second': 1.615, 'total_flos': 9.4907278599168e+16, 'train_loss': 0.012977974861199176, 'epoch': 10.0})"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train(resume_from_checkpoint=False)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82161ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "65891ba7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='353' max='353' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [353/353 00:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.017332835122942924,\n",
       " 'eval_f1_micro': 0.7669086795262849,\n",
       " 'eval_f1_macro': 0.579112270277876,\n",
       " 'eval_roc_auc': 0.894830882110886,\n",
       " 'eval_accuracy': 0.11430599929002484,\n",
       " 'eval_runtime': 49.8318,\n",
       " 'eval_samples_per_second': 226.121,\n",
       " 'eval_steps_per_second': 7.084,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4ad993f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, cc_path(f'models/baselines/paula_finetuned_bert_56k_10e_tka.pt'))\n",
    "\n",
    "# # Predict:\n",
    "# predictions = trainer.predict(encoded_dataset['test'])\n",
    "# print(predictions.predictions.shape, predictions.label_ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e5cf88",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c77b05ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1127/1127 [01:24<00:00, 13.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "with torch.no_grad():\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    model.to(device)\n",
    "    test_dataloader = DataLoader(encoded_dataset[\"test\"], shuffle=False, batch_size=8, collate_fn=data_collator)\n",
    "    outputs = torch.Tensor()\n",
    "    labels = torch.Tensor()\n",
    "    for bi, batch in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        out = model(**batch)\n",
    "        if len(outputs) == 0:\n",
    "            outputs = out.logits\n",
    "            labels = batch['labels']\n",
    "        else:\n",
    "            outputs = torch.cat((outputs, out.logits), 0)\n",
    "            labels = torch.cat((labels, batch['labels']), 0)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "62fa87ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.DataFrame(outputs.cpu().numpy())\n",
    "out.to_csv('out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b90e176e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_micro': 0.7696986502321171,\n",
       " 'f1_macro': 0.5774498492487039,\n",
       " 'roc_auc': 0.8952755695226123,\n",
       " 'accuracy': 0.10849789216773907}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "threshold = 0.5\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs = sigmoid(outputs.to('cpu'))\n",
    "probs\n",
    "# next, use threshold to turn them into integer predictions\n",
    "y_pred = np.zeros(probs.shape)\n",
    "y_pred[np.where(probs >= threshold)] = 1\n",
    "\n",
    "# finally, compute metrics\n",
    "y_true = labels.cpu().numpy()\n",
    "f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "f1_macro_average = f1_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "# return as dictionary\n",
    "metrics = {'f1_micro': f1_micro_average,\n",
    "           'f1_macro': f1_macro_average,\n",
    "           'roc_auc': roc_auc,\n",
    "           'accuracy': accuracy}\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b005bda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(preds, labels, thr=0.5):\n",
    "    '''\n",
    "    Create some metrics: precison, recall, F1...\n",
    "    \n",
    "    A macro-average will compute the metric independently for each class and then take the average hence \n",
    "    treating all classes equally, whereas a micro-average will aggregate the contributions of all classes\n",
    "    to compute the average metric.\n",
    "    '''\n",
    "    \n",
    "    # Convert the lists to dataframes\n",
    "    lab_df = pd.DataFrame(labels.cpu().numpy())\n",
    "    y_pred = np.zeros(preds.shape)\n",
    "    y_pred[np.where(preds >= threshold)] = 1\n",
    "    pred_df = pd.DataFrame(y_pred)\n",
    "    \n",
    "    # Calculate tp/fp/fn/tn per class:\n",
    "    tp = (pred_df + lab_df).eq(2).sum()\n",
    "    fp = (pred_df - lab_df).eq(1).sum()\n",
    "    fn = (pred_df - lab_df).eq(-1).sum()\n",
    "    tn = (pred_df + lab_df).eq(0).sum()\n",
    "    \n",
    "    # Calculate precision and recall:\n",
    "    prec = [tp[i] / (tp[i] + fp[i]) if tp[i] + fp[i] != 0 else 0.0 for i in range(len(tp))]\n",
    "    rec = [tp[i] / (tp[i] + fn[i]) if tp[i] + fn[i] != 0 else 0.0 for i in range(len(tp))]\n",
    "    \n",
    "    # Calculate F1 score:\n",
    "    f1_score = [2 * prec[i] * rec[i] / (prec[i] + rec[i]) if tp[i] > 0 else 0.0 for i in range(len(tp))]\n",
    "    \n",
    "    # Weighted F1 score:\n",
    "    weight = lab_df.sum() / sum(lab_df.sum())\n",
    "    f1_wght = [weight[i] * 2 * prec[i] * rec[i] / (prec[i] + rec[i]) if tp[i] > 0 else 0.0 for i in range(len(tp))]\n",
    "    \n",
    "    # Macro average (average over classes):\n",
    "    prec_avg = sum(prec) / len(prec)\n",
    "    rec_avg = sum(rec) / len(rec)\n",
    "    f1_avg = sum(f1_score) / len(f1_score)\n",
    "    f1wgt_avg = sum(f1_wght)\n",
    "    \n",
    "    # Micro scores (treat all samples together):\n",
    "    tp_mic = sum(tp)\n",
    "    tn_mic = sum(tn)\n",
    "    fp_mic = sum(fp)\n",
    "    fn_mic = sum(fn)\n",
    "    prec_mic = tp_mic / (tp_mic+fp_mic)\n",
    "    rec_mic = tp_mic / (tp_mic+fn_mic)\n",
    "    f1_mic = (2*prec_mic*rec_mic) / (prec_mic+rec_mic)\n",
    "    \n",
    "    return {\n",
    "        'Precision': prec, 'Recall': rec, 'F1 score': f1_score,\n",
    "        'weights': weight, 'Weighted F1 score': f1_wght,\n",
    "        'Macro precision': prec_avg.round(4), 'Macro recall': rec_avg.round(4), 'Macro F1 score': f1_avg.round(4),\n",
    "        'Weighted F1 score': f1wgt_avg.round(4),\n",
    "        'CM TP': tp, 'CM FP': fp,'CM FN': fn, 'CM TN': tn,\n",
    "        'Micro Precision': round(prec_mic, 4), 'Micro Recall': round(rec_mic, 4), 'Micro F1 score': round(f1_mic, 4),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "effd769b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro precision 0.6003\n",
      "Macro recall 0.5858\n",
      "Macro F1 score 0.5774\n",
      "Micro Precision 0.7165\n",
      "Micro Recall 0.8314\n",
      "Micro F1 score 0.7697\n"
     ]
    }
   ],
   "source": [
    "all_metrics = get_metrics(probs, labels)\n",
    "\n",
    "for metr, val in all_metrics.items():\n",
    "    if 'Micro' in metr or 'Macro' in metr:\n",
    "        print(metr, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7c05b2",
   "metadata": {},
   "source": [
    "## train metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "bac13ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4507/4507 [05:39<00:00, 13.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Macro precision 0.7533\n",
      "Macro recall 0.809\n",
      "Macro F1 score 0.7584\n",
      "Micro Precision 0.794\n",
      "Micro Recall 0.9475\n",
      "Micro F1 score 0.864\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "with torch.no_grad():\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    model.to(device)\n",
    "    train_dataloader = DataLoader(encoded_dataset[\"train\"], shuffle=False, batch_size=8, collate_fn=data_collator)\n",
    "    outputs = torch.Tensor()\n",
    "    labels = torch.Tensor()\n",
    "    for bi, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        out = model(**batch)\n",
    "        if len(outputs) == 0:\n",
    "            outputs = out.logits\n",
    "            labels = batch['labels']\n",
    "        else:\n",
    "            outputs = torch.cat((outputs, out.logits), 0)\n",
    "            labels = torch.cat((labels, batch['labels']), 0)\n",
    "print('Done')\n",
    "\n",
    "threshold = 0.5\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs = sigmoid(outputs.to('cpu'))\n",
    "probs\n",
    "# next, use threshold to turn them into integer predictions\n",
    "y_pred = np.zeros(probs.shape)\n",
    "y_pred[np.where(probs >= threshold)] = 1\n",
    "\n",
    "# finally, compute metrics\n",
    "y_true = labels.cpu().numpy()\n",
    "f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "f1_macro_average = f1_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "# return as dictionary\n",
    "metrics = {'f1_micro': f1_micro_average,\n",
    "           'f1_macro': f1_macro_average,\n",
    "           'roc_auc': roc_auc,\n",
    "           'accuracy': accuracy}\n",
    "\n",
    "all_metrics = get_metrics(probs, labels)\n",
    "\n",
    "for metr, val in all_metrics.items():\n",
    "    if 'Micro' in metr or 'Macro' in metr:\n",
    "        print(metr, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11f09f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbc0a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-articleclassifier [Python]",
   "language": "python",
   "name": "conda-env-.conda-articleclassifier-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
