{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Employing GNNs for multi-label classification of biomedical articles Welcome to the documentation of this project. This project focuses on multi-label classification of academic articles using GNNs and label-guided embeddings focused at mitigating the performance decrease result from strongly imbalanced labels. The documentation will cover the different aspects of the project, namely: Setting up the project Creating the right settings Preparing the data Training a model Optimising a model General Remarks This project was created as a 3-month Master Thesis project for the program Information Studies - Data Science. The project was in collaboration with Elsevier under the supervision of Bruno Martins and Chieling Yueh and Prof. Paul Groth.","title":"Index"},{"location":"#employing-gnns-for-multi-label-classification-of-biomedical-articles","text":"Welcome to the documentation of this project. This project focuses on multi-label classification of academic articles using GNNs and label-guided embeddings focused at mitigating the performance decrease result from strongly imbalanced labels. The documentation will cover the different aspects of the project, namely: Setting up the project Creating the right settings Preparing the data Training a model Optimising a model","title":"Employing GNNs for multi-label classification of biomedical articles"},{"location":"#general-remarks","text":"This project was created as a 3-month Master Thesis project for the program Information Studies - Data Science. The project was in collaboration with Elsevier under the supervision of Bruno Martins and Chieling Yueh and Prof. Paul Groth.","title":"General Remarks"},{"location":"optimization/","text":"Optimising a model As each model comprises of separate components, we optimise each component individually. The components in question are: GNN ( graph ) Classification head ( clf_head ) Keyword pruning threshold ( threshold_optimization ) All separate components can be optimised using the --optimize flag and take three arguments: optimization component, dataset, gnn_type E.g.: python main.py --optimize 'clf_head' 'canary' 'GAT' The additional parameters are those set in the default settings files. There is a manual optimization for the threshold available as well, specified with threshold_experiment useful for evaluating the effect of the threshold for specific ranges.","title":"Optimization"},{"location":"optimization/#optimising-a-model","text":"As each model comprises of separate components, we optimise each component individually. The components in question are: GNN ( graph ) Classification head ( clf_head ) Keyword pruning threshold ( threshold_optimization ) All separate components can be optimised using the --optimize flag and take three arguments: optimization component, dataset, gnn_type E.g.: python main.py --optimize 'clf_head' 'canary' 'GAT' The additional parameters are those set in the default settings files. There is a manual optimization for the threshold available as well, specified with threshold_experiment useful for evaluating the effect of the threshold for specific ranges.","title":"Optimising a model"},{"location":"preparing_data/","text":"Preparing data This ReadMe explains how to prepare your data sources from the raw CARs to finetuned embedding text and graph networks. The steps are given sequentially , so follow them in order, only skipping the steps that are not applicable for your dataset as indicated. The choice not to run everything in one single go is based on memory usage and possible errors in between. However, if you are really set on running everything directly after each other, just create a simple bash script. This avoids memory issues and doesn't require adding python code. Processing CAR data ( src/data_preparation/parse_car_xml.py ) Applies to: ['canary'] From the CAR data, we create a single CSV file to work with, extracting the data that we want. Command: python main.py --parse_car_xml Input files: data/raw/canary/original_xml_files/*.xml Output files: data/processed/canary/all_articles_diff_labels.csv Processing text ( src/data_preparation/create_processed_dataset.py ) Applies to: ['canary', 'litcovid'] Clean the text data and format the data to a uniform format Command: python main.py --process_data dataset Input files: raw csv files (e.g. data/processed/canary/all_articles_diff_labels.csv ) Output files: f'data/processed/{dataset}/{dataset}_articles_cleaned_{today}.csv' Creating graph networks ( src/data_preparation/network_generation/create_{network_type}_networks.py ) Applies to: ['canary', 'litcovid'] Create Graph NetworkX structures from the processed data. Command: python main.py --generate_network dataset network_type Implemented network types: ['author', 'keyword', 'label'] Input files: f'data/processed/{dataset}/{dataset}_articles_cleaned_{today}.csv' Output files: f'data/processed/{dataset}/{network_type}_network.pickle' Generating a data split ( src/data_preparation/create_data_split.py ) Applies to: ['canary'] Generates a 64-16-20 data split if the dataset doesn't yet specify a data split. Command: python main.py --create_data_split dataset Generating Embeddings Generate label embedding models ( src/data_preparation/label_embedding/node_to_vec_label_embedding.py ) Applies to: ['canary', 'litcovid'] Embed the label graph to use in the LAHA text embeddings. Command: python main.py --embed_labels dataset Input files: f'data/processed/{dataset}/{dataset}_label_network_weighted.pickle' Training text embedding models Finetuning SciBERT ( src/data_preparation/text_embedding/train_scibert_model.py ) Applies to: ['canary', 'litcovid'] Create a finetuned SciBERT model. Set the file paths to the processed text files. Command: python main.py --train_scibert dataset Input files: f'data/processed/{dataset}/{dataset}_articles_cleaned_{today}.csv' Training SciBERT-LAHA ( src/data_preparation/text_embedding/train_bert_xml_model.py ) Applies to: ['canary', 'litcovid'] Create a trained LAHA model. Set the file paths to the processed text files and finetuned SciBERT model. Command: python main.py --train_xml_embedder dataset Input files: f'data/processed/{dataset}/{dataset}_articles_cleaned_{today}.csv' Generate Text Embeddings SciBERT inference ( src/data_preparation/text_embedding/inference_scibert_model.py ) Applies to: ['canary', 'litcovid'] Generate text embeddings with the finetuned SciBERT model. Set the file paths to the processed text files and finetuned SciBERT model. Command: python main.py --inference_scibert dataset Input files: f'data/processed/{dataset}/{dataset}_articles_cleaned_{today}.csv' SciBERT-LAHA inference ( src/data_preparation/text_embedding/inference_bert_xml_model.py ) Applies to: ['canary', 'litcovid'] Generate text embeddings with the trained LAHA-SciBERT model. Set the file paths to the processed text files and trained LAHA-SciBERT model. Command: python main.py --inference_xml_embedder dataset Input files: f'data/processed/{dataset}/{dataset}_articles_cleaned_{today}.csv'","title":"Data Preparation"},{"location":"preparing_data/#preparing-data","text":"This ReadMe explains how to prepare your data sources from the raw CARs to finetuned embedding text and graph networks. The steps are given sequentially , so follow them in order, only skipping the steps that are not applicable for your dataset as indicated. The choice not to run everything in one single go is based on memory usage and possible errors in between. However, if you are really set on running everything directly after each other, just create a simple bash script. This avoids memory issues and doesn't require adding python code.","title":"Preparing data"},{"location":"preparing_data/#processing-car-data-srcdata_preparationparse_car_xmlpy","text":"Applies to: ['canary'] From the CAR data, we create a single CSV file to work with, extracting the data that we want. Command: python main.py --parse_car_xml Input files: data/raw/canary/original_xml_files/*.xml Output files: data/processed/canary/all_articles_diff_labels.csv","title":"Processing CAR data (src/data_preparation/parse_car_xml.py)"},{"location":"preparing_data/#processing-text-srcdata_preparationcreate_processed_datasetpy","text":"Applies to: ['canary', 'litcovid'] Clean the text data and format the data to a uniform format Command: python main.py --process_data dataset Input files: raw csv files (e.g. data/processed/canary/all_articles_diff_labels.csv ) Output files: f'data/processed/{dataset}/{dataset}_articles_cleaned_{today}.csv'","title":"Processing text (src/data_preparation/create_processed_dataset.py)"},{"location":"preparing_data/#creating-graph-networks-srcdata_preparationnetwork_generationcreate_network_type_networkspy","text":"Applies to: ['canary', 'litcovid'] Create Graph NetworkX structures from the processed data. Command: python main.py --generate_network dataset network_type Implemented network types: ['author', 'keyword', 'label'] Input files: f'data/processed/{dataset}/{dataset}_articles_cleaned_{today}.csv' Output files: f'data/processed/{dataset}/{network_type}_network.pickle'","title":"Creating graph networks (src/data_preparation/network_generation/create_{network_type}_networks.py)"},{"location":"preparing_data/#generating-a-data-split-srcdata_preparationcreate_data_splitpy","text":"Applies to: ['canary'] Generates a 64-16-20 data split if the dataset doesn't yet specify a data split. Command: python main.py --create_data_split dataset","title":"Generating a data split (src/data_preparation/create_data_split.py)"},{"location":"preparing_data/#generating-embeddings","text":"","title":"Generating Embeddings"},{"location":"preparing_data/#generate-label-embedding-models-srcdata_preparationlabel_embeddingnode_to_vec_label_embeddingpy","text":"Applies to: ['canary', 'litcovid'] Embed the label graph to use in the LAHA text embeddings. Command: python main.py --embed_labels dataset Input files: f'data/processed/{dataset}/{dataset}_label_network_weighted.pickle'","title":"Generate label embedding models (src/data_preparation/label_embedding/node_to_vec_label_embedding.py)"},{"location":"preparing_data/#training-text-embedding-models","text":"","title":"Training text embedding models"},{"location":"preparing_data/#finetuning-scibert-srcdata_preparationtext_embeddingtrain_scibert_modelpy","text":"Applies to: ['canary', 'litcovid'] Create a finetuned SciBERT model. Set the file paths to the processed text files. Command: python main.py --train_scibert dataset Input files: f'data/processed/{dataset}/{dataset}_articles_cleaned_{today}.csv'","title":"Finetuning SciBERT (src/data_preparation/text_embedding/train_scibert_model.py)"},{"location":"preparing_data/#training-scibert-laha-srcdata_preparationtext_embeddingtrain_bert_xml_modelpy","text":"Applies to: ['canary', 'litcovid'] Create a trained LAHA model. Set the file paths to the processed text files and finetuned SciBERT model. Command: python main.py --train_xml_embedder dataset Input files: f'data/processed/{dataset}/{dataset}_articles_cleaned_{today}.csv'","title":"Training SciBERT-LAHA (src/data_preparation/text_embedding/train_bert_xml_model.py)"},{"location":"preparing_data/#generate-text-embeddings","text":"","title":"Generate Text Embeddings"},{"location":"preparing_data/#scibert-inference-srcdata_preparationtext_embeddinginference_scibert_modelpy","text":"Applies to: ['canary', 'litcovid'] Generate text embeddings with the finetuned SciBERT model. Set the file paths to the processed text files and finetuned SciBERT model. Command: python main.py --inference_scibert dataset Input files: f'data/processed/{dataset}/{dataset}_articles_cleaned_{today}.csv'","title":"SciBERT inference (src/data_preparation/text_embedding/inference_scibert_model.py)"},{"location":"preparing_data/#scibert-laha-inference-srcdata_preparationtext_embeddinginference_bert_xml_modelpy","text":"Applies to: ['canary', 'litcovid'] Generate text embeddings with the trained LAHA-SciBERT model. Set the file paths to the processed text files and trained LAHA-SciBERT model. Command: python main.py --inference_xml_embedder dataset Input files: f'data/processed/{dataset}/{dataset}_articles_cleaned_{today}.csv'","title":"SciBERT-LAHA inference (src/data_preparation/text_embedding/inference_bert_xml_model.py)"},{"location":"project_installation/","text":"Project Installation The project environment may be set up using: conda create --name articleclassifier --file requirements.txt Confirm afterwards that the GPU configuration has succeeded (if actually using Nvidia GPUs, but if not, you should get trained models from someone, because training without GPUs takes ages.) You can do a quick check by looking at the output of the following code: print('__Python VERSION:', sys.version) print('__pyTorch VERSION:', torch.__version__) print('__CUDA VERSION', ) from subprocess import call # call([\"nvcc\", \"--version\"]) does not work ! nvcc - -version print('__CUDNN VERSION:', torch.backends.cudnn.version()) print('__Number CUDA Devices:', torch.cuda.device_count()) print('__Devices') # call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"]) print('Active CUDA Device: GPU', torch.cuda.current_device()) print('Available devices ', torch.cuda.device_count())","title":"Setting up"},{"location":"project_installation/#project-installation","text":"The project environment may be set up using: conda create --name articleclassifier --file requirements.txt Confirm afterwards that the GPU configuration has succeeded (if actually using Nvidia GPUs, but if not, you should get trained models from someone, because training without GPUs takes ages.) You can do a quick check by looking at the output of the following code: print('__Python VERSION:', sys.version) print('__pyTorch VERSION:', torch.__version__) print('__CUDA VERSION', ) from subprocess import call # call([\"nvcc\", \"--version\"]) does not work ! nvcc - -version print('__CUDNN VERSION:', torch.backends.cudnn.version()) print('__Number CUDA Devices:', torch.cuda.device_count()) print('__Devices') # call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"]) print('Active CUDA Device: GPU', torch.cuda.current_device()) print('Available devices ', torch.cuda.device_count())","title":"Project Installation"},{"location":"settings/","text":"Configuring the right settings The default settings are set in the .yaml files in the src/default_settings/ folder. These settings are used throughout the code and are stored in a Configuration object as defined in src/general/settings.py . Please refer to these files for setttings on: Graph Model hyperparameters Graph training parameters Classification head hyperparameters Data hyperparameters Pretrain settings File locations","title":"Configuring Settings"},{"location":"settings/#configuring-the-right-settings","text":"The default settings are set in the .yaml files in the src/default_settings/ folder. These settings are used throughout the code and are stored in a Configuration object as defined in src/general/settings.py . Please refer to these files for setttings on: Graph Model hyperparameters Graph training parameters Classification head hyperparameters Data hyperparameters Pretrain settings File locations","title":"Configuring the right settings"},{"location":"training/","text":"Training This ReadMe explains how to train and evaluate a GNN on the data using the command line. Before training a model make sure the environment is built from the requirements.txt file and activated. build the datasets as described in the data_preparation folder Training a model Training a new model is very straightforward. After setting the preferred settings as per the 'settings' page, you can run: python main.py --run_model Running a trained model In case you want to rerun a trained model, you can add the id of the model: python main.py --run_model --model_id model_id","title":"Training"},{"location":"training/#training","text":"This ReadMe explains how to train and evaluate a GNN on the data using the command line.","title":"Training"},{"location":"training/#before-training-a-model","text":"make sure the environment is built from the requirements.txt file and activated. build the datasets as described in the data_preparation folder","title":"Before training a model"},{"location":"training/#training-a-model","text":"Training a new model is very straightforward. After setting the preferred settings as per the 'settings' page, you can run: python main.py --run_model","title":"Training a model"},{"location":"training/#running-a-trained-model","text":"In case you want to rerun a trained model, you can add the id of the model: python main.py --run_model --model_id model_id","title":"Running a trained model"},{"location":"welcome/","text":"Welcome to the GNN article classification repo! Testing the docs here. Viewing the documentation mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Welcome to the GNN article classification repo!"},{"location":"welcome/#welcome-to-the-gnn-article-classification-repo","text":"Testing the docs here.","title":"Welcome to the GNN article classification repo!"},{"location":"welcome/#viewing-the-documentation","text":"mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Viewing the documentation"}]}