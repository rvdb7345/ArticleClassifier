{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Employing GNNs for multi-label classification of biomedical articles","text":"<p>Welcome to the documentation of this project.</p> <p>This project focuses on the multi-label classification of academic articles using GNNs and label-guided embeddings aimed at mitigating the performance decrease resulting from strongly imbalanced labels.</p> <p>The documentation will cover the different aspects of the project, namely:</p> <ul> <li>Setting up the project</li> <li>Creating the right settings</li> <li>Preparing the data</li> <li>Training a model</li> <li>Optimising a model</li> </ul> <p></p>"},{"location":"#general-remarks","title":"General Remarks","text":"<p>This project was created as a 3-month Master's Thesis project for the program Information Studies - Data Science. The project was done in collaboration with Elsevier under the supervision of Bruno Martins, Chieling Yueh and Prof. dr. Paul Groth.</p>"},{"location":"#contact","title":"Contact","text":"<p>If you have any questions or comments regarding this project, contact me through:</p> <ul> <li>LinkedIn</li> <li>Email: rvdb7345@gmail.com</li> </ul>"},{"location":"optimization/","title":"Optimising a model","text":"<p>As each model comprises of separate components, we optimise each component individually. The components in question are:</p> <ul> <li>GNN (<code>graph</code>)</li> <li>Classification head (<code>clf_head</code>)</li> <li>Keyword pruning threshold (<code>threshold_optimization</code>)</li> </ul> <p>All separate components can be optimised using the <code>--optimize</code> flag and take three arguments: optimization component, dataset, gnn_type</p> <p>E.g.:</p> <pre><code>python main.py --optimize 'clf_head' 'canary' 'GAT'\n</code></pre> <p>The additional parameters are those set in the default settings files.</p> <p>There is a manual optimization for the threshold available as well, specified with <code>threshold_experiment</code> useful for evaluating the effect of the threshold for specific ranges.</p>"},{"location":"preparing_data/","title":"Preparing data","text":"<p>This ReadMe explains how to prepare your data sources from the raw CARs to finetuned embedding text and graph networks. The steps are given sequentially, so follow them in order, only skipping the steps that are not applicable for your dataset as indicated.</p> <p>The choice not to run everything in one single go is based on memory usage and possible errors in between. However, if you are really set on running everything directly after each other, just create a simple bash script. This avoids memory issues and doesn't require adding python code.</p>"},{"location":"preparing_data/#processing-car-data","title":"Processing CAR data","text":"<p>(<code>src/data_preparation/parse_car_xml.py</code>)</p> <p>Applies to: ['canary']</p> <p>From the CAR data, we create a single CSV file to work with, extracting the data that we want.</p> <p>Command: <code>python main.py --parse_car_xml</code></p> <p>Input files: <code>data/raw/canary/original_xml_files/*.xml</code></p> <p>Output files: <code>data/processed/canary/all_articles_diff_labels.csv</code></p>"},{"location":"preparing_data/#processing-text","title":"Processing text","text":"<p>(<code>src/data_preparation/create_processed_dataset.py</code>)</p> <p>Applies to: ['canary', 'litcovid']</p> <p>Clean the text data and format the data to a uniform format</p> <p>Command:</p> <pre><code>python main.py --process_data dataset\n</code></pre> <p>Input files: raw csv files (e.g. <code>data/processed/canary/all_articles_diff_labels.csv</code>)</p> <p>Output files: <code>f'data/processed/{dataset}/{dataset}_articles_cleaned_{today}.csv'</code></p>"},{"location":"preparing_data/#creating-graph-networks","title":"Creating graph networks","text":"<p>(<code>src/data_preparation/network_generation/create_{network_type}_networks.py</code>)</p> <p>Applies to: ['canary', 'litcovid']</p> <p>Create Graph NetworkX structures from the processed data.</p> <p>Command:</p> <pre><code>python main.py --generate_network dataset network_type\n</code></pre> <p>Implemented network types: ['author', 'keyword', 'label']</p> <p>Input files: <code>f'data/processed/{dataset}/{dataset}_articles_cleaned_{today}.csv'</code></p> <p>Output files: <code>f'data/processed/{dataset}/{network_type}_network.pickle'</code></p>"},{"location":"preparing_data/#generating-a-data-split","title":"Generating a data split","text":"<p>(<code>src/data_preparation/create_data_split.py</code>)</p> <p>Applies to: ['canary']</p> <p>Generates a 64-16-20 data split if the dataset doesn't yet specify a data split.</p> <p>Command:</p> <pre><code>python main.py --create_data_split dataset\n</code></pre>"},{"location":"preparing_data/#generating-embeddings","title":"Generating Embeddings","text":""},{"location":"preparing_data/#generate-label-embedding-models","title":"Generate label embedding models","text":"<p>(<code>src/data_preparation/label_embedding/node_to_vec_label_embedding.py</code>)</p> <p>Applies to: ['canary', 'litcovid']</p> <p>Embed the label graph to use in the LAHA text embeddings.</p> <p>Command:</p> <pre><code>python main.py --embed_labels dataset\n</code></pre> <p>Input files: <code>f'data/processed/{dataset}/{dataset}_label_network_weighted.pickle'</code></p>"},{"location":"preparing_data/#training-text-embedding-models","title":"Training text embedding models","text":""},{"location":"preparing_data/#finetuning-scibert","title":"Finetuning SciBERT","text":"<p>(<code>src/data_preparation/text_embedding/train_scibert_model.py</code>)</p> <p>Applies to: ['canary', 'litcovid']</p> <p>Create a finetuned SciBERT model. Set the file paths to the processed text files.</p> <p>Command:</p> <pre><code>python main.py --train_scibert dataset\n</code></pre> <p>Input files: <code>f'data/processed/{dataset}/{dataset}_articles_cleaned_{today}.csv'</code></p>"},{"location":"preparing_data/#training-scibert-laha","title":"Training SciBERT-LAHA","text":"<p>(<code>src/data_preparation/text_embedding/train_bert_xml_model.py</code>)</p> <p>Applies to: ['canary', 'litcovid']</p> <p>Create a trained LAHA model. Set the file paths to the processed text files and finetuned SciBERT model.</p> <p>Command:</p> <pre><code>python main.py --train_xml_embedder dataset\n</code></pre> <p>Input files: <code>f'data/processed/{dataset}/{dataset}_articles_cleaned_{today}.csv'</code></p>"},{"location":"preparing_data/#generate-text-embeddings","title":"Generate Text Embeddings","text":""},{"location":"preparing_data/#scibert-inference","title":"SciBERT inference","text":"<p>(<code>src/data_preparation/text_embedding/inference_scibert_model.py</code>)</p> <p>Applies to: ['canary', 'litcovid']</p> <p>Generate text embeddings with the finetuned SciBERT model. Set the file paths to the processed text files and finetuned SciBERT model.</p> <p>Command:</p> <pre><code>python main.py --inference_scibert dataset\n</code></pre> <p>Input files: <code>f'data/processed/{dataset}/{dataset}_articles_cleaned_{today}.csv'</code></p>"},{"location":"preparing_data/#scibert-laha-inference","title":"SciBERT-LAHA inference","text":"<p>(<code>src/data_preparation/text_embedding/inference_bert_xml_model.py</code>)</p> <p>Applies to: ['canary', 'litcovid']</p> <p>Generate text embeddings with the trained LAHA-SciBERT model. Set the file paths to the processed text files and trained LAHA-SciBERT model.</p> <p>Command:</p> <pre><code>python main.py --inference_xml_embedder dataset\n</code></pre> <p>Input files: <code>f'data/processed/{dataset}/{dataset}_articles_cleaned_{today}.csv'</code></p>"},{"location":"project_installation/","title":"Project Installation","text":"<p>The project environment may be set up using: <code>conda create --name articleclassifier --file requirements.txt</code></p> <p>Confirm afterwards that the GPU configuration has succeeded (if actually using Nvidia GPUs, but if not, you should get trained models from someone, because training without GPUs takes ages.) You can do a quick check by looking at the output of the following code:</p> <pre><code>print('__Python VERSION:', sys.version)\nprint('__pyTorch VERSION:', torch.__version__)\nprint('__CUDA VERSION', )\nfrom subprocess import call\n# call([\"nvcc\", \"--version\"]) does not work\n! nvcc - -version\nprint('__CUDNN VERSION:', torch.backends.cudnn.version())\nprint('__Number CUDA Devices:', torch.cuda.device_count())\nprint('__Devices')\n# call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\nprint('Active CUDA Device: GPU', torch.cuda.current_device())\nprint('Available devices ', torch.cuda.device_count())\n</code></pre>"},{"location":"settings/","title":"Configuring the right settings","text":"<p>The default settings are set in the <code>.yaml</code> files in the <code>src/default_settings/</code> folder. These settings are used throughout the code and are stored in a <code>Configuration</code> object as defined in <code>src/general/settings.py</code>.</p> <p>Please refer to these files for setttings on:</p> <ul> <li>Graph Model hyperparameters</li> <li>Graph training parameters</li> <li>Classification head hyperparameters</li> <li>Data hyperparameters</li> <li>Pretrain settings</li> <li>File locations</li> </ul>"},{"location":"training/","title":"Training","text":"<p>This ReadMe explains how to train and evaluate a GNN on the data using the command line.</p>"},{"location":"training/#before-training-a-model","title":"Before training a model","text":"<ul> <li>make sure the environment is built from the requirements.txt file and activated.</li> <li>build the datasets as described in the data_preparation folder</li> </ul>"},{"location":"training/#training-a-model","title":"Training a model","text":"<p>Training a new model is very straightforward. After setting the preferred settings as per the 'settings' page, you can run:</p> <pre><code>python main.py --run_model 'canary' 'GAT'\n</code></pre>"},{"location":"training/#running-a-trained-model","title":"Running a trained model","text":"<p>In case you want to rerun a trained model, you can add the id of the model:</p> <pre><code>python main.py --run_model 'litcovid' 'GAT' --model_id model_id\n</code></pre>"},{"location":"welcome/","title":"Welcome to the GNN article classification repo!","text":"<p>Testing the docs here.</p>"},{"location":"welcome/#viewing-the-documentation","title":"Viewing the documentation","text":"<ul> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"}]}